{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4840bd8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:17.835198Z",
     "iopub.status.busy": "2025-01-13T17:40:17.834837Z",
     "iopub.status.idle": "2025-01-13T17:40:23.328486Z",
     "shell.execute_reply": "2025-01-13T17:40:23.327588Z"
    },
    "papermill": {
     "duration": 5.503866,
     "end_time": "2025-01-13T17:40:23.330181",
     "exception": false,
     "start_time": "2025-01-13T17:40:17.826315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1736790023.323508"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc, os, copy, torch, pickle, bisect, logging, warnings\n",
    "print(torch.cuda.is_available())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# os.environ[\"POLARS_MAX_THREADS\"] = \"1\"\n",
    "import polars as pl\n",
    "print(pl.thread_pool_size())\n",
    "import sys \n",
    "import joblib\n",
    "import kaggle_evaluation.jane_street_inference_server as js_server\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Literal\n",
    "from typing import Tuple, Union, List\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Manager\n",
    "pd.set_option('display.max_colwidth', 2000)\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # 设置日志级别\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",  # 设置日志格式\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",  # 设置时间格式\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94612b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.345623Z",
     "iopub.status.busy": "2025-01-13T17:40:23.344916Z",
     "iopub.status.idle": "2025-01-13T17:40:23.349889Z",
     "shell.execute_reply": "2025-01-13T17:40:23.349170Z"
    },
    "papermill": {
     "duration": 0.013486,
     "end_time": "2025-01-13T17:40:23.351354",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.337868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    seed = 666\n",
    "    path = \"/kaggle/input/jane-street-real-time-market-data-forecasting\"\n",
    "\n",
    "class SpecialCols:\n",
    "    date_id = \"date_id\"\n",
    "    time_id = \"time_id\"\n",
    "    symbol_id = \"symbol_id\"\n",
    "    weight_col = \"weight\"\n",
    "    id_cols = [date_id, time_id, symbol_id]\n",
    "    target_col = \"responder_6\"\n",
    "    target_cols = [\"responder_%d\" % i for i in range(9)]\n",
    "    feature_cols = [f\"feature_{i:02}\"  for i in range(79)]\n",
    "    target_lag1d_cols =  [\"responder_%d_lag1d\" % i for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3f471e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.364713Z",
     "iopub.status.busy": "2025-01-13T17:40:23.364166Z",
     "iopub.status.idle": "2025-01-13T17:40:23.368995Z",
     "shell.execute_reply": "2025-01-13T17:40:23.368371Z"
    },
    "papermill": {
     "duration": 0.013254,
     "end_time": "2025-01-13T17:40:23.370659",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.357405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timing_decorator_with_params(name=\"\"):\n",
    "    \"\"\"\n",
    "    装饰器工厂函数，接受参数并返回装饰器。\n",
    "    :param display_result: 是否显示函数的返回结果。\n",
    "    \"\"\"\n",
    "    def timing_decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()  # 记录开始时间\n",
    "            result = func(*args, **kwargs)  # 执行函数\n",
    "            end_time = time.time()  # 记录结束时间\n",
    "            print(f\"Function '{name}--{func.__name__}' executed in {end_time - start_time:.6f} seconds\")\n",
    "\n",
    "            return result\n",
    "        return wrapper\n",
    "    return timing_decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef1e10",
   "metadata": {
    "papermill": {
     "duration": 0.005917,
     "end_time": "2025-01-13T17:40:23.382713",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.376796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73431019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.396200Z",
     "iopub.status.busy": "2025-01-13T17:40:23.395974Z",
     "iopub.status.idle": "2025-01-13T17:40:23.431410Z",
     "shell.execute_reply": "2025-01-13T17:40:23.430597Z"
    },
    "papermill": {
     "duration": 0.044373,
     "end_time": "2025-01-13T17:40:23.433041",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.388668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lazy_sort_index(df: pd.DataFrame, axis=0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    make the df index sorted\n",
    "\n",
    "    df.sort_index() will take a lot of time even when `df.is_lexsorted() == True`\n",
    "    This function could avoid such case\n",
    "\n",
    "    \"\"\"\n",
    "    idx = df.index if axis == 0 else df.columns\n",
    "    if (\n",
    "        not idx.is_monotonic_increasing\n",
    "        and isinstance(idx, pd.MultiIndex)\n",
    "        and not idx.is_lexsorted()\n",
    "    ):  \n",
    "        return df.sort_index(axis=axis)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def np_ffill(arr: np.array):\n",
    "    \"\"\"\n",
    "    forward fill a 1D numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.array\n",
    "        Input numpy 1D array\n",
    "    \"\"\"\n",
    "    mask = np.isnan(arr.astype(float))  # np.isnan only works on np.float\n",
    "    # get fill index\n",
    "    idx = np.where(~mask, np.arange(mask.shape[0]), 0)\n",
    "    np.maximum.accumulate(idx, out=idx)\n",
    "    return arr[idx]\n",
    "\n",
    "class TSDataSamplerJ:\n",
    "    \"\"\"\n",
    "    (T)ime-(S)eries DataSampler\n",
    "    This is the result of TSDatasetH\n",
    "\n",
    "    It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series\n",
    "    dataset based on tabular data.\n",
    "    - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future\n",
    "      data.\n",
    "\n",
    "    If user have further requirements for processing data, user could process them based on `TSDataSampler` or create\n",
    "    more powerful subclasses.\n",
    "\n",
    "    Known Issues:\n",
    "    - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result\n",
    "      in a different data type\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, data: pd.DataFrame, start, end, step_len: int, fillna_type: str = \"none\", dtype=None, flt_data=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build a dataset which looks like torch.data.utils.Dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            The raw tabular data\n",
    "        start :\n",
    "            The indexable start time\n",
    "        end :\n",
    "            The indexable end time\n",
    "        step_len : int\n",
    "            The length of the time-series step\n",
    "        fillna_type : int\n",
    "            How will qlib handle the sample if there is on sample in a specific date.\n",
    "            none:\n",
    "                fill with np.nan\n",
    "            ffill:\n",
    "                ffill with previous sample\n",
    "            ffill+bfill:\n",
    "                ffill with previous samples first and fill with later samples second\n",
    "        flt_data : pd.Series\n",
    "            a column of data(True or False) to filter data.\n",
    "            None:\n",
    "                kepp all data\n",
    "\n",
    "        \"\"\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.step_len = step_len\n",
    "        self.fillna_type = fillna_type\n",
    "        self.data = lazy_sort_index(data)\n",
    "\n",
    "        kwargs = {\"object\": self.data}\n",
    "        if dtype is not None:\n",
    "            kwargs[\"dtype\"] = dtype\n",
    "\n",
    "        self.data_arr = np.array(**kwargs)  # Get index from numpy.array will much faster than DataFrame.values!\n",
    "        # NOTE:\n",
    "        # - append last line with full NaN for better performance in `__getitem__`\n",
    "        # - Keep the same dtype will result in a better performance\n",
    "        self.data_arr = np.append(\n",
    "            self.data_arr, np.full((1, self.data_arr.shape[1]), np.nan, dtype=self.data_arr.dtype), axis=0\n",
    "        )\n",
    "        self.nan_idx = -1  # The last line is all NaN\n",
    "\n",
    "        # the data type will be changed\n",
    "        # The index of usable data is between start_idx and end_idx\n",
    "        self.idx_df, self.idx_map = self.build_index(self.data)\n",
    "        self.data_index = deepcopy(self.data.index)\n",
    "\n",
    "        if flt_data is not None:\n",
    "            if isinstance(flt_data, pd.DataFrame):\n",
    "                assert len(flt_data.columns) == 1\n",
    "                flt_data = flt_data.iloc[:, 0]\n",
    "            # NOTE: bool(np.nan) is True !!!!!!!!\n",
    "            # make sure reindex comes first. Otherwise extra NaN may appear.\n",
    "            flt_data = flt_data.reindex(self.data_index).fillna(False).astype(np.bool)\n",
    "            self.flt_data = flt_data.values\n",
    "            self.idx_map = self.flt_idx_map(self.flt_data, self.idx_map)\n",
    "            self.data_index = self.data_index[np.where(self.flt_data)[0]]\n",
    "        self.idx_map = self.idx_map2arr(self.idx_map)\n",
    "\n",
    "        self.start_idx, self.end_idx = self.data_index.slice_locs(\n",
    "            start=start, end=end\n",
    "        )\n",
    "        self.idx_arr = np.array(self.idx_df.values, dtype=np.float64)  # for better performance\n",
    "\n",
    "        # del self.data  # save memory\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_map2arr(idx_map):\n",
    "        # pytorch data sampler will have better memory control without large dict or list\n",
    "        # - https://github.com/pytorch/pytorch/issues/13243\n",
    "        # - https://github.com/airctic/icevision/issues/613\n",
    "        # So we convert the dict into int array.\n",
    "        # The arr_map is expected to behave the same as idx_map\n",
    "\n",
    "        dtype = np.int64\n",
    "        # set a index out of bound to indicate the none existing\n",
    "        no_existing_idx = (np.iinfo(dtype).max, np.iinfo(dtype).max)\n",
    "\n",
    "        max_idx = max(idx_map.keys())\n",
    "        arr_map = []\n",
    "        for i in range(max_idx + 1):\n",
    "            arr_map.append(idx_map.get(i, no_existing_idx))\n",
    "        arr_map = np.array(arr_map, dtype=dtype)\n",
    "        return arr_map\n",
    "\n",
    "    @staticmethod\n",
    "    def flt_idx_map(flt_data, idx_map):\n",
    "        idx = 0\n",
    "        new_idx_map = {}\n",
    "        for i, exist in enumerate(flt_data):\n",
    "            if exist:\n",
    "                new_idx_map[idx] = idx_map[i]\n",
    "                idx += 1\n",
    "        return new_idx_map\n",
    "\n",
    "    def get_index(self):\n",
    "        \"\"\"\n",
    "        Get the pandas index of the data, it will be useful in following scenarios\n",
    "        - Special sampler will be used (e.g. user want to sample day by day)\n",
    "        \"\"\"\n",
    "        return self.data_index[self.start_idx : self.end_idx]\n",
    "\n",
    "    def config(self, **kwargs):\n",
    "        # Config the attributes\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_index(data: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:\n",
    "        \"\"\"\n",
    "        The relation of the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            The dataframe with <datetime, DataFrame>\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[pd.DataFrame, dict]:\n",
    "            1) the first element:  reshape the original index into a <datetime(row), instrument(column)> 2D dataframe\n",
    "                instrument SH600000 SH600004 SH600006 SH600007 SH600008 SH600009  ...\n",
    "                datetime\n",
    "                2021-01-11        0        1        2        3        4        5  ...\n",
    "                2021-01-12     4146     4147     4148     4149     4150     4151  ...\n",
    "                2021-01-13     8293     8294     8295     8296     8297     8298  ...\n",
    "                2021-01-14    12441    12442    12443    12444    12445    12446  ...\n",
    "            2) the second element:  {<original index>: <row, col>}\n",
    "        \"\"\"\n",
    "        # object incase of pandas converting int to float\n",
    "        idx_df = pd.Series(range(data.shape[0]), index=data.index, dtype=object)\n",
    "        idx_df = lazy_sort_index(idx_df.unstack())\n",
    "        # NOTE: the correctness of `__getitem__` depends on columns sorted here\n",
    "        idx_df = lazy_sort_index(idx_df, axis=1)\n",
    "\n",
    "        idx_map = {}\n",
    "        for i, (_, row) in enumerate(idx_df.iterrows()):\n",
    "            for j, real_idx in enumerate(row):\n",
    "                if not np.isnan(real_idx):\n",
    "                    idx_map[real_idx] = (i, j)\n",
    "        return idx_df, idx_map\n",
    "\n",
    "    @property\n",
    "    def empty(self):\n",
    "        return len(self) == 0\n",
    "\n",
    "    def _get_indices(self, row: int, col: int) -> np.array:\n",
    "        \"\"\"\n",
    "        get series indices of self.data_arr from the row, col indices of self.idx_df\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        row : int\n",
    "            the row in self.idx_df\n",
    "        col : int\n",
    "            the col in self.idx_df\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array:\n",
    "            The indices of data of the data\n",
    "        \"\"\"\n",
    "        indices = self.idx_arr[max(row - self.step_len + 1, 0) : row + 1, col]\n",
    "\n",
    "        if len(indices) < self.step_len:\n",
    "            indices = np.concatenate([np.full((self.step_len - len(indices),), np.nan), indices])\n",
    "\n",
    "        if self.fillna_type == \"ffill\":\n",
    "            indices = np_ffill(indices)\n",
    "        elif self.fillna_type == \"ffill+bfill\":\n",
    "            indices = np_ffill(np_ffill(indices)[::-1])[::-1]\n",
    "        else:\n",
    "            assert self.fillna_type == \"none\"\n",
    "        return indices\n",
    "\n",
    "    def _get_row_col(self, idx) -> Tuple[int]:\n",
    "        \"\"\"\n",
    "        get the col index and row index of a given sample index in self.idx_df\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx :\n",
    "            the input of  `__getitem__`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[int]:\n",
    "            the row and col index\n",
    "        \"\"\"\n",
    "        # The the right row number `i` and col number `j` in idx_df\n",
    "        if isinstance(idx, (int, np.integer)):\n",
    "            real_idx = self.start_idx + idx\n",
    "            if self.start_idx <= real_idx < self.end_idx:\n",
    "                i, j = self.idx_map[real_idx]  # TODO: The performance of this line is not good\n",
    "            else:\n",
    "                raise KeyError(f\"{real_idx} is out of [{self.start_idx}, {self.end_idx})\")\n",
    "        elif isinstance(idx, tuple):\n",
    "            # <TSDataSampler object>[\"datetime\", \"instruments\"]\n",
    "            date, inst = idx\n",
    "            date = pd.Timestamp(date)\n",
    "            i = bisect.bisect_right(self.idx_df.index, date) - 1\n",
    "            # NOTE: This relies on the idx_df columns sorted in `__init__`\n",
    "            j = bisect.bisect_left(self.idx_df.columns, inst)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"This type of input is not supported\")\n",
    "        return i, j\n",
    "\n",
    "    def __getitem__(self, idx: Union[int, Tuple[object, str], List[int]]):\n",
    "        \"\"\"\n",
    "        # We have two method to get the time-series of a sample\n",
    "        tsds is a instance of TSDataSampler\n",
    "\n",
    "        # 1) sample by int index directly\n",
    "        tsds[len(tsds) - 1]\n",
    "\n",
    "        # 2) sample by <datetime,instrument> index\n",
    "        tsds['2016-12-31', \"SZ300315\"]\n",
    "\n",
    "        # The return value will be similar to the data retrieved by following code\n",
    "        df.loc(axis=0)['2015-01-01':'2016-12-31', \"SZ300315\"].iloc[-30:]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : Union[int, Tuple[object, str]]\n",
    "        \"\"\"\n",
    "        # Multi-index type\n",
    "        mtit = (list, np.ndarray)\n",
    "        if isinstance(idx, mtit):\n",
    "            indices = [self._get_indices(*self._get_row_col(i)) for i in idx]\n",
    "            indices = np.concatenate(indices)\n",
    "        else:\n",
    "            indices = self._get_indices(*self._get_row_col(idx))\n",
    "\n",
    "        # 1) for better performance, use the last nan line for padding the lost date\n",
    "        # 2) In case of precision problems. We use np.float64. # TODO: I'm not sure if whether np.float64 will result in\n",
    "        # precision problems. It will not cause any problems in my tests at least\n",
    "        indices = np.nan_to_num(indices.astype(np.float64), nan=self.nan_idx).astype(int)\n",
    "\n",
    "        data = self.data_arr[indices]\n",
    "        if isinstance(idx, mtit):\n",
    "            # if we get multiple indexes, addition dimension should be added.\n",
    "            # <sample_idx, step_idx, feature_idx>\n",
    "            data = data.reshape(-1, self.step_len, *data.shape[1:])\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.start_idx\n",
    "\n",
    "class DailyBatchSamplerRandom(Sampler):\n",
    "    def __init__(self, data_source, shuffle=False, get_last_batch=False):\n",
    "        self.data_source = data_source\n",
    "        self.shuffle = shuffle\n",
    "        # calculate number of samples in each batch\n",
    "        self.daily_count = pd.Series(index=self.data_source.get_index(), dtype=pd.Float32Dtype).groupby(\"time_id\").size().values\n",
    "        self.daily_index = np.roll(np.cumsum(self.daily_count), 1)  # calculate begin index of each batch\n",
    "        self.daily_index[0] = 0\n",
    "        self.get_last_batch = get_last_batch\n",
    "\n",
    "    def __iter__(self):\n",
    "        index = np.arange(len(self.daily_count))\n",
    "        if self.get_last_batch:\n",
    "            index = index[::-1]\n",
    "        for i in index:\n",
    "            yield np.arange(self.daily_index[i], self.daily_index[i] + self.daily_count[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "\n",
    "# 包装DataLoader生成4维输入\n",
    "def batch_combine(data_loader, batch_size, drop_last=True):\n",
    "    \"\"\"\n",
    "    将 DataLoader 的输出组合成批次，确保每个批次的股票数量 N 一致。\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): PyTorch 数据加载器。\n",
    "        batch_size (int): 合并的批次大小。\n",
    "\n",
    "    Yields:\n",
    "        torch.Tensor: 组合后的批次，形状为 [batch_size, N, step_len, feature_dim]。\n",
    "    \"\"\"\n",
    "    batch_buffer = []  # 用于暂时存储每个小批次的数据\n",
    "    current_N = None   # 当前批次的股票数\n",
    "\n",
    "    for data in data_loader:\n",
    "        # data.shape: [1, N, step_len, feature_dim]\n",
    "        _, N, _, _ = data.shape\n",
    "\n",
    "        if current_N is None:\n",
    "            # 初始化 current_N\n",
    "            current_N = N\n",
    "\n",
    "        if N == current_N:\n",
    "            # 当前批次的 N 与记录一致，将数据添加到 buffer\n",
    "            batch_buffer.append(data)\n",
    "\n",
    "            if len(batch_buffer) == batch_size:\n",
    "                # 达到目标批次数量，合并并返回\n",
    "                combined_batch = torch.cat(batch_buffer, dim=0)  # 合并成 [batch_size, N, step_len, feature_dim]\n",
    "                yield combined_batch\n",
    "                batch_buffer = []  # 清空 buffer\n",
    "                current_N = None  # 重置 current_N\n",
    "\n",
    "        else:\n",
    "            # 当前 N 与记录不一致，先处理已有 buffer\n",
    "            if batch_buffer:\n",
    "                combined_batch = torch.cat(batch_buffer, dim=0)\n",
    "                yield combined_batch\n",
    "                batch_buffer = []\n",
    "\n",
    "            # 更新 current_N 并重新开始\n",
    "            current_N = N\n",
    "            batch_buffer.append(data)\n",
    "\n",
    "    # 处理剩余的 buffer（如果有），TOFIX，有bug，一定要drop_last\n",
    "    if batch_buffer:\n",
    "        if not drop_last:\n",
    "            combined_batch = torch.cat(batch_buffer, dim=0)\n",
    "            yield combined_batch\n",
    "\n",
    "def get_dataloader(data: pl.DataFrame, step_len, get_last_batch=False):\n",
    "    \"\"\"\n",
    "    data的格式 = dp.get()\n",
    "    \"\"\"\n",
    "    # 获取DataLoader\n",
    "    # counts = data.groupby('date_id')['time_id'].max() + 1\n",
    "    # offsets = counts.shift(fill_value=0).cumsum()\n",
    "    # data['offset'] = data['date_id'].map(offsets)\n",
    "    # data['time_id'] += data['offset']\n",
    "    # data.drop(columns=['offset', 'date_id'], inplace=True)\n",
    "    # data.set_index(['time_id', 'symbol_id'], inplace=True)\n",
    "    # # 重新排列列顺序\n",
    "    # notin = ['weight', ] + ['responder_{}'.format(i) for i in range(0, 9)]\n",
    "    # columns_order = [col for col in data.columns if col not in notin] + notin\n",
    "    # data = data[columns_order].sort_index()\n",
    "    # # 生成dataset\n",
    "    # min_time_id = data.index.get_level_values('time_id').min()\n",
    "    # max_time_id = data.index.get_level_values('time_id').max()\n",
    "\n",
    "    data = data.sort([SpecialCols.date_id, SpecialCols.time_id])\n",
    "    counts = data.group_by(\"date_id\").agg(pl.col(\"time_id\").max()).with_columns(pl.col(\"time_id\") + 1)\n",
    "    offsets = counts.with_columns(pl.col(\"time_id\").shift(1, fill_value=0).cum_sum().alias(\"offset\"))\n",
    "    data = data.join(offsets.select([\"date_id\", \"offset\"]), on=\"date_id\")\n",
    "    data = data.with_columns((pl.col(\"time_id\") + pl.col(\"offset\")).alias(\"time_id\"))\n",
    "    data = data.drop([\"offset\", \"date_id\"])\n",
    "    data = data.sort([\"time_id\", \"symbol_id\"])\n",
    "    # 重新排列列顺序\n",
    "    notin = [\"weight\"] + [f\"responder_{i}\" for i in range(9)]\n",
    "    columns_order = [col for col in data.columns if col not in notin] + notin\n",
    "    data = data.select(columns_order)\n",
    "    \n",
    "    # 获取 time_id 的最小值和最大值\n",
    "    min_time_id = data.select(pl.col(\"time_id\").min()).item()\n",
    "    max_time_id = data.select(pl.col(\"time_id\").max()).item()\n",
    "    data = data.to_pandas().set_index(['time_id', 'symbol_id'])\n",
    "\n",
    "    # 生成DataLoader\n",
    "    dataset = TSDataSamplerJ(data=data, start=min_time_id, end=max_time_id, step_len=step_len, fillna_type='ffill+bfill')\n",
    "    sampler = DailyBatchSamplerRandom(dataset, shuffle=False, get_last_batch=get_last_batch)\n",
    "    loader = DataLoader(dataset, sampler=sampler) #, num_workers=4)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d689d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.446561Z",
     "iopub.status.busy": "2025-01-13T17:40:23.446053Z",
     "iopub.status.idle": "2025-01-13T17:40:23.451247Z",
     "shell.execute_reply": "2025-01-13T17:40:23.450470Z"
    },
    "papermill": {
     "duration": 0.013455,
     "end_time": "2025-01-13T17:40:23.452813",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.439358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模拟接口\n",
    "def generate_data_batches(test_path, lags_path):\n",
    "    date_ids = sorted(\n",
    "        pl.scan_parquet(test_path)\n",
    "        .select(pl.col(\"date_id\").unique())\n",
    "        .collect()\n",
    "        .get_column(\"date_id\")\n",
    "    )\n",
    "    assert date_ids[0] == 0\n",
    "\n",
    "    for date_id in date_ids:\n",
    "        test_batches = pl.read_parquet(\n",
    "            os.path.join(test_path, f\"date_id={date_id}\"),\n",
    "        ).group_by(\"time_id\", maintain_order=True)\n",
    "\n",
    "        lags = pl.read_parquet(\n",
    "            os.path.join(lags_path, f\"date_id={date_id}\"),\n",
    "        )\n",
    "\n",
    "        for (time_id,), test in test_batches:\n",
    "            test_data = (test, lags if time_id == 0 else None)\n",
    "            validation_data = test.select('row_id')\n",
    "            yield test_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41f5918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.467306Z",
     "iopub.status.busy": "2025-01-13T17:40:23.466992Z",
     "iopub.status.idle": "2025-01-13T17:40:23.667951Z",
     "shell.execute_reply": "2025-01-13T17:40:23.666977Z"
    },
    "papermill": {
     "duration": 0.211042,
     "end_time": "2025-01-13T17:40:23.670054",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.459012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义一下剔除的特征，因为.pkl没有修改\n",
    "eliminated_feats = ['feature_20_beta', 'feature_24_beta', 'feature_25_beta', 'feature_27_beta', 'feature_29_beta', 'responder_3_dailymean_lag1d_beta', 'responder_5_dailymean_lag1d_beta', 'responder_8_dailymean_lag1d_beta', 'responder_8_dailymean_lag1d_residual']+['feature_22_avg_same_timeid', 'feature_31_avg_same_timeid', 'feature_58_std_same_timeid', 'feature_60_std_same_timeid', 'feature_61_avg_same_timeid', 'feature_66_avg_same_timeid', 'feature_67_avg_same_timeid',\n",
    "                                                                                                                                                                                                                                                                        'responder_6_lag1d_avg_same_timeid', 'responder_7_lag1d_avg_same_timeid', 'weight_avg_same_timeid']+['feature_05_beta_mean_lag_1_d', 'feature_06_beta_mean_lag_1_d', 'feature_06_beta_std_lag_1_d', 'feature_07_beta_std_lag_1_d', 'feature_16_beta_std_lag_1_d', 'feature_21_beta_std_lag_1_d', 'feature_37_residual_std_lag_1_d', 'feature_47_residual_mean_lag_1_d', 'feature_54_beta_last_lag_1_d', 'feature_56_beta_mean_lag_1_d', 'feature_57_beta_mean_lag_1_d', 'feature_61_beta_std_lag_1_d']\n",
    "\n",
    "class DailyFeature:\n",
    "    \"\"\"一支票一个对象\"\"\"\n",
    "\n",
    "    def __init__(self, instrument_id, time_id, window, features_n):\n",
    "        \"\"\"定义存储的数据\n",
    "        Args:\n",
    "            instrument_id (str): 标的的id\n",
    "            window (int): 窗口的大小\n",
    "            features_n(int) : 特征数\n",
    "        \"\"\"\n",
    "        self.instrument_id = instrument_id\n",
    "        self.time_id = time_id\n",
    "        self.window = window\n",
    "        self.features_n = features_n\n",
    "        self.empty_result = np.full((1, features_n), np.nan)\n",
    "\n",
    "    def load_new_value(self):\n",
    "        \"\"\"传入新的数据\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_cur_result(self):\n",
    "        \"\"\"定义计算逻辑\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TSMEAN_STD(DailyFeature):\n",
    "    def __init__(self, instrument_id, time_id, window, features_n):\n",
    "        super().__init__(instrument_id, time_id, window, features_n)\n",
    "\n",
    "        # 初始化当前的数据\n",
    "        self.x_sum = np.empty(shape=(0, features_n))\n",
    "        self.xx_sum = np.empty(shape=(0, features_n))\n",
    "        self.save_window = self.window + 1\n",
    "\n",
    "        self.n = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def load_new_value(self, xs_features_data: np.ndarray):\n",
    "        \"\"\"传入新的数据\"\"\"\n",
    "        if self.n < self.window:\n",
    "            self.n += 1\n",
    "        self.count += 1\n",
    "\n",
    "        # 防止溢出\n",
    "        if self.count % self.window == 0:\n",
    "            self.x_sum = self.x_sum - self.x_sum[0]\n",
    "            self.xx_sum = self.xx_sum - self.xx_sum[0]\n",
    "\n",
    "        # 填充nan为0\n",
    "        xs_features_data = np.nan_to_num(xs_features_data, nan=0)\n",
    "        assert self.x_sum.shape[1] == xs_features_data.shape[1]\n",
    "        # 如果已有数据, 取cumsum后拼接\n",
    "        if self.x_sum.shape[0] > 0:\n",
    "            self.x_sum = np.concatenate(\n",
    "                (self.x_sum, self.x_sum[-1] + xs_features_data), axis=0\n",
    "            )[-self.save_window:]\n",
    "            self.xx_sum = np.concatenate(\n",
    "                (self.xx_sum, self.xx_sum[-1] +\n",
    "                 xs_features_data * xs_features_data),\n",
    "                axis=0,\n",
    "            )[-self.save_window:]\n",
    "        # 没有数据，直接拼接即可\n",
    "        else:\n",
    "            self.x_sum = np.concatenate((self.x_sum, xs_features_data), axis=0)[\n",
    "                -self.save_window:\n",
    "            ]\n",
    "            self.xx_sum = np.concatenate(\n",
    "                (self.xx_sum, xs_features_data * xs_features_data), axis=0\n",
    "            )[-self.save_window:]\n",
    "\n",
    "    def get_cur_result(self):\n",
    "        if self.x_sum.shape[0] <= 1:\n",
    "            return self.empty_result, self.empty_result\n",
    "        else:\n",
    "            n = self.n\n",
    "            temp = self.x_sum[-1] - self.x_sum[0]\n",
    "            mean = temp / n\n",
    "\n",
    "            std = np.sqrt(\n",
    "                (self.xx_sum[-1] - self.xx_sum[0]) /\n",
    "                (n) - temp * temp / (n * n)\n",
    "            )\n",
    "\n",
    "            return mean, std\n",
    "\n",
    "class HFDataProcessor:\n",
    "    col_orders = ['row_id'] + SpecialCols.id_cols + [SpecialCols.weight_col] + ['is_scored'] + \\\n",
    "        SpecialCols.feature_cols + SpecialCols.target_lag1d_cols\n",
    "    all_selected_feats = ['feature_00_market_simple_mean', 'feature_05_market_weighted_mean', 'feature_37_market_weighted_mean', 'feature_60_market_weighted_sum', 'feature_09_market_weighted_sum', 'feature_39_market_simple_mean', 'feature_39_market_weighted_mean', 'feature_10_market_weighted_sum', 'feature_01_market_weighted_sum', 'feature_58_market_weighted_sum', 'feature_62_market_weighted_mean', 'feature_45_market_simple_mean', 'feature_29_market_weighted_sum', 'feature_04_market_weighted_mean', 'feature_26_market_simple_mean', 'feature_60_market_simple_mean', 'feature_37_market_simple_mean', 'feature_42_market_simple_mean', 'feature_57_market_simple_mean', 'feature_29_market_simple_mean', 'feature_01_market_simple_mean', 'feature_73_market_simple_mean', 'feature_18_market_simple_mean', 'feature_11_market_weighted_sum', 'feature_46_market_simple_mean', 'feature_07_market_weighted_mean', 'feature_09_market_simple_mean', 'feature_21_market_simple_mean', 'feature_06_market_weighted_mean', 'feature_01_market_weighted_mean', 'feature_11_market_simple_mean', 'feature_02_market_simple_mean', 'feature_53_market_simple_mean', 'feature_78_market_simple_mean', 'feature_04_market_simple_mean', 'feature_59_market_weighted_sum', 'feature_49_market_simple_mean', 'feature_03_market_weighted_mean', 'feature_38_market_simple_mean', 'feature_05_market_simple_mean', 'feature_02_market_weighted_mean', 'feature_56_market_weighted_sum', 'feature_33_market_simple_mean', 'feature_49_market_weighted_sum', 'feature_53_market_weighted_mean', 'feature_08_market_simple_mean', 'feature_04_market_weighted_sum', 'feature_20_market_simple_mean', 'feature_08_market_weighted_mean', 'feature_37_market_weighted_sum', 'feature_59_market_weighted_sum_gpby_feature_09', 'feature_06_market_simple_mean_gpby_feature_09', 'feature_47_market_weighted_sum_gpby_feature_09', 'feature_58_market_weighted_sum_gpby_feature_09', 'feature_38_market_weighted_mean_gpby_feature_09', 'feature_49_market_simple_mean_gpby_feature_09', 'feature_68_market_weighted_sum_gpby_feature_09', 'feature_59_market_weighted_mean_gpby_feature_09', 'feature_48_market_weighted_sum_gpby_feature_09', 'feature_06_market_weighted_sum_gpby_feature_09', 'feature_38_market_weighted_sum_gpby_feature_09', 'feature_30_market_weighted_sum_gpby_feature_09',\n",
    "                          'feature_60_market_weighted_sum_gpby_feature_09', 'feature_56_market_weighted_sum_gpby_feature_09', 'feature_56_market_simple_mean_gpby_feature_09', 'feature_59_weighted_signal', 'feature_36_weighted_signal', 'feature_31_weighted_signal', 'feature_56_weighted_signal', 'weight_weighted_signal', 'feature_72_weighted_signal', 'feature_60_weighted_signal', 'feature_45_weighted_signal', 'feature_55_weighted_signal', 'feature_09_weighted_signal', 'feature_58_weighted_signal', 'feature_07_last_lag_1_d', 'feature_01_mean_lag_1_d', 'feature_01_std_lag_1_d', 'feature_16_std_lag_1_d', 'feature_38_mean_lag_1_d', 'feature_08_mean_lag_1_d', 'feature_02_std_lag_1_d', 'feature_04_mean_lag_1_d', 'feature_05_mean_lag_1_d', 'feature_30_mean_lag_1_d', 'feature_04_std_lag_1_d', 'feature_05_last_lag_1_d', 'feature_38_last_lag_1_d', 'feature_61_mean_lag_1_d', 'feature_72_avg_same_symbol', 'feature_07_avg_same_symbol', 'feature_59_avg_same_symbol', 'feature_56_avg_same_symbol', 'feature_41_avg_same_symbol', 'feature_16_std_same_symbol', 'feature_37_avg_same_symbol', 'feature_06_std_same_symbol', 'feature_55_avg_same_symbol', 'feature_59_std_same_symbol', 'feature_52_std_same_symbol', 'feature_36_avg_same_symbol', 'feature_07_std_same_symbol', 'feature_58_avg_same_symbol', 'feature_19_avg_same_symbol', 'feature_08_avg_same_symbol', 'feature_02_avg_same_symbol', 'feature_68_avg_same_symbol', 'feature_38_std_same_symbol', 'feature_30_avg_same_symbol', 'feature_37_std_same_symbol', 'feature_52_avg_same_symbol', 'feature_18_avg_same_symbol', 'feature_51_avg_same_symbol', 'feature_38_avg_same_symbol', 'feature_70_avg_same_symbol', 'feature_05_avg_same_symbol', 'feature_58_std_same_symbol', 'feature_48_std_same_symbol', 'feature_15_avg_same_symbol', 'feature_54_avg_same_symbol', 'feature_01_avg_same_symbol', 'weight_avg_same_symbol', 'feature_04_avg_same_symbol', 'feature_66_avg_same_symbol', 'feature_00_avg_same_symbol', 'feature_57_avg_same_symbol', 'feature_60_avg_same_symbol', 'feature_60_std_same_symbol', 'feature_30_simple_lag_1', 'feature_59_simple_lag_1', 'feature_60_simple_lag_1', 'feature_69_simple_lag_1', 'feature_58_simple_lag_1', 'feature_07_simple_lag_1', 'feature_25_market_simple_mean_gpby_feature_10', 'feature_49_market_weighted_sum_gpby_feature_10', 'feature_24_market_simple_mean_gpby_feature_10', 'feature_07_zscore']\n",
    "    function_to_use = pd.read_pickle(\n",
    "        r\"/kaggle/input/aft-js-dataset/function_to_use.pkl\"\n",
    "    )\n",
    "\n",
    "    def __init__(self, ts_window=100) -> None:\n",
    "        # logger.info(\"data preparing\")\n",
    "        # 原始数据\n",
    "        self._data: pl.DataFrame = None\n",
    "        # self._data_pd: pd.DataFrame = None\n",
    "\n",
    "        self.return_data: pl.DataFrame = None\n",
    "        self.time_queue = []\n",
    "\n",
    "        self._keep_length = 6000  # 原始留存多少行的数据\n",
    "        self._seq_len = 16  # 返回的截面数\n",
    "        self._ts_window = ts_window\n",
    "\n",
    "        # ffill相关\n",
    "        self.ffill_values: pl.DataFrame = None\n",
    "        # 最新的经过easy_process_raw_data处理的raw_data\n",
    "        self.cur_processed_raw_data: pl.DataFrame = None\n",
    "\n",
    "        # 存每一个time_id 的concat(原始特征，数据)\n",
    "        self._cache = []\n",
    "\n",
    "        # 下面的参数与日间数据特征计算有关\n",
    "        self.symbols = set()\n",
    "        self.new_symbols = set()\n",
    "        self.tsmean_stder = dict()\n",
    "\n",
    "        # 每日计算的特征\n",
    "        self.ts_daily_lag_res = None\n",
    "\n",
    "        self.clustered_features = {\n",
    "            'class_1': ['feature_09', 'feature_10', 'feature_11', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31'],\n",
    "            'class_2': ['feature_07', 'feature_08', 'feature_37', 'feature_38', 'feature_41', 'feature_45', 'feature_49', 'feature_51', 'feature_52', 'feature_55', 'feature_56', 'feature_60'],\n",
    "            'class_3': ['feature_02', 'feature_04', 'feature_06', 'feature_34', 'feature_36'],\n",
    "            'class_4': ['feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78'],\n",
    "            'class_5': ['feature_15', 'feature_16', 'feature_17', 'feature_61', 'feature_62', 'feature_63', 'feature_64'],\n",
    "            'class_6': ['feature_05', 'feature_39', 'feature_42', 'feature_47', 'feature_50', 'feature_53', 'feature_58'],\n",
    "            'class_7': ['feature_40', 'feature_43', 'feature_44', 'feature_46', 'feature_48', 'feature_54', 'feature_57', 'feature_59'],\n",
    "            'class_8': ['feature_18', 'feature_19', 'feature_65', 'feature_66'],\n",
    "            'class_9': ['feature_00', 'feature_01', 'feature_03', 'feature_32', 'feature_33', 'feature_35'],\n",
    "            'class_10': ['feature_12', 'feature_13', 'feature_14', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72']\n",
    "        }\n",
    "        self.pca_models = {\n",
    "            class_name: joblib.load(\n",
    "                f'/kaggle/input/aft-js-dataset/pca_model_f79/pca_model_1_{class_name}.pkl')\n",
    "            for class_name in self.clustered_features.keys()\n",
    "        }  # load 模型很慢 提前准备好\n",
    "\n",
    "    def add_pca(self, new_data):\n",
    "        for class_name, class_list in self.clustered_features.items():\n",
    "            reduction_feature_values = new_data[class_list].to_numpy()\n",
    "            new_factor = self.pca_models[class_name].transform(\n",
    "                reduction_feature_values)\n",
    "            new_data = new_data.with_columns(\n",
    "                pl.Series(f\"pca_{class_name}\", new_factor.reshape(-1)))\n",
    "        return new_data\n",
    "\n",
    "    @staticmethod\n",
    "    def easy_process_raw_data(data: pl.DataFrame, lag1_responder: pl.DataFrame):\n",
    "        \"\"\"\n",
    "        data: 原始数据带responder(可以填0)\n",
    "        lag1_responder: column为responder,值滞后一天.\n",
    "        \"\"\"\n",
    "        lag1_responder = lag1_responder.rename(\n",
    "            {col: col + \"_lag1d\" for col in SpecialCols.target_cols}\n",
    "        )\n",
    "        data = data.join(lag1_responder, on=SpecialCols.id_cols, how=\"left\")[\n",
    "            HFDataProcessor.col_orders]\n",
    "\n",
    "        return data\n",
    "\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def _trans_raw_data_2pd(self, data: pl.DataFrame):\n",
    "        \"\"\"讲raw_data转化为pd.DataFrame\"\"\"\n",
    "        data = data.to_pandas()\n",
    "        data = data.set_index(SpecialCols.id_cols).sort_index()\n",
    "        data[\"time_id_col\"] = data.index.get_level_values(SpecialCols.time_id)\n",
    "        data[\"symbol_id_col\"] = data.index.get_level_values(\n",
    "            SpecialCols.symbol_id)\n",
    "        data[\"date_id_col\"] = data.index.get_level_values(SpecialCols.date_id)\n",
    "        return data\n",
    "\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def _trans_back_2pl(self, data: pd.DataFrame):\n",
    "        \"\"\"将reg的result转化回pl.DataFrame,添加上ids_col\"\"\"\n",
    "        return pl.DataFrame(data.reset_index()).with_columns(pl.col(SpecialCols.symbol_id).cast(pl.Int8))\n",
    "\n",
    "    #\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def xs_signal_weighted(\n",
    "        self,\n",
    "        feats: List[str] = None,\n",
    "    ) -> pl.DataFrame:\n",
    "        pl_data = (\n",
    "            self.cur_processed_raw_data[SpecialCols.id_cols + feats]\n",
    "            if feats is not None\n",
    "            else self.cur_processed_raw_data.clone()\n",
    "        )\n",
    "\n",
    "        feat_cols = [\n",
    "            feat for feat in pl_data.columns if feat not in SpecialCols.id_cols]\n",
    "\n",
    "        cols, data = feat_cols, pl_data[feat_cols].to_numpy()\n",
    "        row_medians = np.median(data, axis=0, keepdims=True)\n",
    "        normalized_data = data - row_medians\n",
    "        positive_sums = np.nansum(\n",
    "            np.where(normalized_data > 0, normalized_data, 0), axis=0)\n",
    "        negative_sums = np.nansum(\n",
    "            np.where(normalized_data < 0, normalized_data, 0), axis=0)\n",
    "        result = np.where(\n",
    "            normalized_data > 0, normalized_data / positive_sums,\n",
    "            np.where(normalized_data < 0, - normalized_data / negative_sums, 0)\n",
    "        )\n",
    "        result = pl.DataFrame(result)\n",
    "        result.columns = [col + '_weighted_signal' for col in cols]\n",
    "        result = pl.concat(\n",
    "            [pl_data[SpecialCols.id_cols], result], how='horizontal')\n",
    "        return result\n",
    "\n",
    "    #\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def xs_market(\n",
    "        self,\n",
    "        feats: List[str] = None,\n",
    "        groupby_col: Literal[\"feature_09\", \"feature_10\"] = None,\n",
    "    ) -> pl.DataFrame:\n",
    "        assert groupby_col in [\n",
    "            None,\n",
    "            \"feature_09\",\n",
    "            \"feature_10\",\n",
    "        ], \"groupby_col is invalid\"\n",
    "        if (groupby_col is not None) and (groupby_col not in feats):\n",
    "            feats.append(groupby_col)\n",
    "        data = (\n",
    "            self.cur_processed_raw_data[SpecialCols.id_cols +\n",
    "                                        [SpecialCols.weight_col] + feats]\n",
    "            if feats is not None\n",
    "            else self.cur_processed_raw_data.clone()\n",
    "        )\n",
    "\n",
    "        feat_cols = [\n",
    "            feat\n",
    "            for feat in data.columns\n",
    "            if feat not in [SpecialCols.weight_col] + SpecialCols.id_cols\n",
    "        ]\n",
    "\n",
    "        # 剔除groupby_col\n",
    "        if groupby_col is not None:\n",
    "            feat_cols = [col for col in feat_cols if col != groupby_col]\n",
    "\n",
    "        standard_weight = data[SpecialCols.weight_col] / \\\n",
    "            data[SpecialCols.weight_col].sum()\n",
    "\n",
    "        standrad_data = data.with_columns(\n",
    "            [(pl.col(col) * standard_weight) for col in feat_cols])\n",
    "\n",
    "        if groupby_col is not None:\n",
    "            reshaped_mean = data[feat_cols + [groupby_col]].group_by(groupby_col, maintain_order=True).mean().rename(\n",
    "                lambda column_name: column_name + \"_market_simple_mean_gpby_\" + groupby_col if column_name != groupby_col else column_name)\n",
    "            reshaped_weighted_mean = standrad_data.group_by(groupby_col, maintain_order=True).mean(\n",
    "            ).rename(lambda column_name: column_name + \"_market_weighted_mean_gpby_\" + groupby_col)\n",
    "            reshaped_weighted_sum = standrad_data.group_by(groupby_col, maintain_order=True).sum(\n",
    "            ).rename(lambda column_name: column_name + \"_market_weighted_sum_gpby_\" + groupby_col)\n",
    "            output = pl.concat([reshaped_mean, reshaped_weighted_mean, reshaped_weighted_sum],\n",
    "                               how='horizontal').with_columns(pl.col(groupby_col).cast(data[groupby_col].dtype))\n",
    "            return data.select(SpecialCols.id_cols + [groupby_col]).join(output, how='left', on=groupby_col)\n",
    "        else:\n",
    "            reshaped_mean = data[feat_cols + [\"time_id\"]].mean().rename(lambda column_name: column_name +\n",
    "                                                                        \"_market_simple_mean\" if column_name != \"time_id\" else column_name)\n",
    "            reshaped_weighted_mean = standrad_data.mean().rename(\n",
    "                lambda column_name: column_name + \"_market_weighted_mean\")\n",
    "            reshaped_weighted_sum = standrad_data.sum().rename(\n",
    "                lambda column_name: column_name + \"_market_weighted_sum\")\n",
    "            output = pl.concat([reshaped_mean, reshaped_weighted_mean, reshaped_weighted_sum],\n",
    "                               how='horizontal').with_columns(pl.col(\"time_id\").cast(data[\"time_id\"].dtype))\n",
    "            return data.select(SpecialCols.id_cols).join(output, how='left', on=\"time_id\")\n",
    "    #\n",
    "\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def ts_mean_std_same_symbol(self, feats: List[str] = None) -> pl.DataFrame:\n",
    "        data = (\n",
    "            self._data[SpecialCols.id_cols + feats]\n",
    "            if feats is not None\n",
    "            else self._data.copy()\n",
    "        )\n",
    "        # 剔除target,id_col\n",
    "        feat_cols = [\n",
    "            feat\n",
    "            for feat in data.columns\n",
    "            if feat not in SpecialCols.target_cols + SpecialCols.id_cols\n",
    "        ]\n",
    "\n",
    "        # 只选取需要的数据, 因为是日内进行, 所以选取一天即可\n",
    "        data = data.filter((pl.col(SpecialCols.date_id) == self.date_id))\n",
    "\n",
    "        max_time_id = data[SpecialCols.time_id].max()\n",
    "        if max_time_id < 29:\n",
    "            # 这里返回的是空的\n",
    "            return data.filter(pl.col(SpecialCols.time_id) == max_time_id + 1).with_columns([\n",
    "                pl.col(col)\n",
    "                .alias(f\"{col}_avg_same_symbol\")\n",
    "                for col in feat_cols\n",
    "            ]).with_columns([\n",
    "                pl.col(col)\n",
    "                .alias(f\"{col}_std_same_symbol\")\n",
    "                for col in feat_cols\n",
    "            ])\n",
    "        else:\n",
    "            data = data.filter(pl.col(SpecialCols.time_id) >= max_time_id - 29)\n",
    "            return data.group_by([SpecialCols.symbol_id]).agg(\n",
    "                [pl.col(col).mean().alias(f\"{col}_avg_same_symbol\") for col in feat_cols]+[\n",
    "                    pl.col(col).std().alias(f\"{col}_std_same_symbol\") for col in feat_cols]\n",
    "            ).with_columns(\n",
    "                [pl.lit(self.date_id).cast(pl.Int16).alias(SpecialCols.date_id), pl.lit(\n",
    "                    self.time_id).cast(pl.Int16).alias(SpecialCols.time_id)]\n",
    "            )\n",
    "            # return data.with_columns([\n",
    "            #     pl.col(col).mean().over([SpecialCols.symbol_id])\n",
    "            #     .alias(f\"{col}_avg_same_symbol\")\n",
    "            #     for col in feat_cols\n",
    "            # ]).with_columns([\n",
    "            #     pl.col(col).std().over([SpecialCols.symbol_id])\n",
    "            #     .alias(f\"{col}_std_same_symbol\")\n",
    "            #     for col in feat_cols\n",
    "            # ]).filter((pl.col(SpecialCols.time_id) == self.time_id))\n",
    "\n",
    "    #\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def ts_mean_std_same_timeid(self, feats: List[str] = None) -> pl.DataFrame:\n",
    "        data = (\n",
    "            self._data[SpecialCols.id_cols + feats]\n",
    "            if feats is not None\n",
    "            else self._data.copy()\n",
    "        )\n",
    "        # 剔除weight, target,id_col\n",
    "        feat_cols = [\n",
    "            feat\n",
    "            for feat in data.columns\n",
    "            if feat not in SpecialCols.target_cols + SpecialCols.id_cols\n",
    "        ]\n",
    "\n",
    "        self._update_tsmean_std(self.new_symbols, feats=feat_cols)\n",
    "\n",
    "        result = []\n",
    "        # for symbol_id_time_id, obj in self.tsmean_stder.items():\n",
    "        for symbol_id in self.new_symbols:\n",
    "            obj = self.tsmean_stder[f\"{symbol_id}_{self.time_id}\"]\n",
    "            ids = pl.DataFrame(\n",
    "                [[self.date_id], [self.time_id], [symbol_id]],\n",
    "                schema={\n",
    "                    SpecialCols.date_id: pl.Int16,\n",
    "                    SpecialCols.time_id: pl.Int16,\n",
    "                    SpecialCols.symbol_id: pl.Int8,\n",
    "                },\n",
    "            )\n",
    "            mean, std = obj.get_cur_result()\n",
    "            mean = pl.DataFrame(mean.reshape(1, -1), schema=feat_cols).rename(\n",
    "                {col: col + \"_avg_same_timeid\" for col in feat_cols}\n",
    "            )\n",
    "            std = pl.DataFrame(std.reshape(1, -1), schema=feat_cols).rename(\n",
    "                {col: col + \"_std_same_timeid\" for col in feat_cols}\n",
    "            )\n",
    "\n",
    "            # 标识结果\n",
    "            one_id_result = pl.concat([ids, mean, std], how=\"horizontal\")\n",
    "            result.append(one_id_result)\n",
    "\n",
    "        result = pl.concat(result, how=\"vertical\")\n",
    "        return result\n",
    "        \"\"\"\n",
    "        # 需要30天的数据,但是time_id相同,\n",
    "        data = data.filter((pl.col(SpecialCols.time_id) == self.time_id))\n",
    "\n",
    "        data = data.with_columns([pl.col(col).rolling_mean(30).over(\n",
    "            [SpecialCols.time_id, SpecialCols.symbol_id]).alias(f\"{col}_avg_same_timeid\") for col in feat_cols])\n",
    "\n",
    "        data = data.with_columns([pl.col(col).rolling_std(30).over(\n",
    "            [SpecialCols.time_id, SpecialCols.symbol_id]).alias(f\"{col}_std_same_timeid\") for col in feat_cols])\n",
    "        return data.filter((pl.col(SpecialCols.date_id) == self.date_id))\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # v\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def ts_simple_lag(self, feats: List[str] = None) -> pl.DataFrame:\n",
    "        data = (\n",
    "            self._data[SpecialCols.id_cols + feats]\n",
    "            if feats is not None\n",
    "            else self._data.copy()\n",
    "        )\n",
    "        # 获取特征列\n",
    "        feat_cols = [\n",
    "            feat\n",
    "            for feat in data.columns\n",
    "            if feat not in SpecialCols.target_cols + SpecialCols.id_cols\n",
    "        ]\n",
    "        # 只选取需要的数据, 因为是日内lag，所以选取一天即可\n",
    "        data = data.filter((pl.col(SpecialCols.date_id) == self.date_id) & (\n",
    "            (pl.col(SpecialCols.time_id) >= self.time_id-2)))\n",
    "\n",
    "        lags = data.with_columns(\n",
    "            [\n",
    "                pl.col(col)\n",
    "                .shift(1)\n",
    "                .over(([SpecialCols.date_id, SpecialCols.symbol_id]))\n",
    "                .alias(col + \"_simple_lag_1\")\n",
    "                for col in feat_cols\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return lags.filter(pl.col(SpecialCols.time_id) == self.time_id)\n",
    "\n",
    "    # v\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def ts_daily_lag(self, feats: List[str] = None, lag_n_day=1) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        ['feature_30_mean_lag_1_d', 'feature_04_mean_lag_1_d', 'feature_05_mean_lag_1_d', 'feature_07_last_lag_1_d', 'feature_08_mean_lag_1_d', 'feature_02_std_lag_1_d', 'feature_16_std_lag_1_d', 'feature_38_mean_lag_1_d', 'feature_04_std_lag_1_d', 'feature_38_last_lag_1_d', 'feature_61_mean_lag_1_d', 'feature_05_last_lag_1_d', 'feature_01_mean_lag_1_d', 'feature_01_std_lag_1_d']\n",
    "        \"\"\"\n",
    "        last_data_file = f\"/kaggle/working/data_of_{self.date_id-1}.parquet\"\n",
    "        if self.time_id != 0 or not os.path.exists(last_data_file):\n",
    "            if self.ts_daily_lag_res is None:\n",
    "                data = (\n",
    "                    self._data[SpecialCols.id_cols + feats]\n",
    "                    if feats is not None\n",
    "                    else self._data.copy()\n",
    "                )\n",
    "                # 剔除weight, target, id_col\n",
    "                feat_cols = [\n",
    "                    feat\n",
    "                    for feat in data.columns\n",
    "                    if feat not in SpecialCols.target_cols + SpecialCols.id_cols\n",
    "                ]\n",
    "                lags_shift = data.filter(\n",
    "                    (pl.col(SpecialCols.date_id) == self.date_id - lag_n_day)\n",
    "                )\n",
    "\n",
    "                # 重设使其变为当前天数\n",
    "                lags_shift = lags_shift.with_columns(\n",
    "                    pl.lit(self.date_id).alias(\n",
    "                        SpecialCols.date_id).cast(pl.Int16)\n",
    "                )\n",
    "\n",
    "                # 没有找到agg传str的方法 只有写3个agg函数\n",
    "                lags_shift = lags_shift.with_columns(\n",
    "                    [\n",
    "                        pl.col(col)\n",
    "                        .mean()\n",
    "                        .over([SpecialCols.symbol_id])\n",
    "                        .alias(f\"{col}_mean_lag_{lag_n_day}_d\")\n",
    "                        for col in feat_cols\n",
    "                    ]\n",
    "                )\n",
    "                lags_shift = lags_shift.with_columns(\n",
    "                    [\n",
    "                        pl.col(col)\n",
    "                        .std()\n",
    "                        .over(([SpecialCols.symbol_id]))\n",
    "                        .alias(f\"{col}_std_lag_{lag_n_day}_d\")\n",
    "                        for col in feat_cols\n",
    "                    ]\n",
    "                )\n",
    "                lags_shift = lags_shift.with_columns(\n",
    "                    [\n",
    "                        pl.col(col)\n",
    "                        .last()\n",
    "                        .over(([SpecialCols.symbol_id]))\n",
    "                        .alias(f\"{col}_last_lag_{lag_n_day}_d\")\n",
    "                        for col in feat_cols\n",
    "                    ]\n",
    "                )\n",
    "                self.ts_daily_lag_res = lags_shift\n",
    "                return lags_shift.filter(\n",
    "                    (pl.col(SpecialCols.time_id) == self.time_id))\n",
    "\n",
    "            return self.ts_daily_lag_res.filter(\n",
    "                (pl.col(SpecialCols.time_id) == self.time_id))\n",
    "        data = pl.read_parquet(\n",
    "            last_data_file, columns=SpecialCols.id_cols + feats)\n",
    "\n",
    "        last_data = data[[SpecialCols.symbol_id] + feats]\n",
    "        last = last_data.group_by('symbol_id').tail(1).rename(\n",
    "            lambda column_name: column_name + \"_last_lag_1_d\" if column_name != 'symbol_id' else column_name)\n",
    "        mean = last_data.group_by('symbol_id').mean()\n",
    "        square_mean = last_data.with_columns([pl.col(\n",
    "            f) * pl.col(f) for f in last_data.columns if f != 'symbol_id']).group_by('symbol_id').mean()\n",
    "\n",
    "        std = square_mean.with_columns([pl.col(f) - mean[f] * mean[f] for f in last_data.columns if f != 'symbol_id']).rename(\n",
    "            lambda column_name: column_name + \"_std_lag_1_d\" if column_name != 'symbol_id' else column_name)\n",
    "\n",
    "        mean = mean.rename(lambda column_name: column_name +\n",
    "                           \"_mean_lag_1_d\" if column_name != 'symbol_id' else column_name)\n",
    "\n",
    "        self.ts_daily_lag_res = data.join(\n",
    "            last.join(mean, on='symbol_id', how='inner').join(\n",
    "                std, on='symbol_id', how='inner'),\n",
    "            on='symbol_id', how='left'\n",
    "        ).with_columns([pl.col(f).cast(pl.Float32) for f in ['feature_30_mean_lag_1_d', 'feature_04_mean_lag_1_d', 'feature_05_mean_lag_1_d', 'feature_08_mean_lag_1_d', 'feature_02_std_lag_1_d', 'feature_16_std_lag_1_d', 'feature_38_mean_lag_1_d', 'feature_04_std_lag_1_d', 'feature_61_mean_lag_1_d', 'feature_01_mean_lag_1_d', 'feature_01_std_lag_1_d']])\n",
    "        \n",
    "\n",
    "        return self.ts_daily_lag_res.filter(\n",
    "            (pl.col(SpecialCols.time_id) == self.time_id))  # 只获取当前的time_id\n",
    "    # v\n",
    "\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def ts_macross(self, feats: List[str] = None) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        目标： 生成时序上MA5和MA15的百分比距离\n",
    "        1. 对每个特征进行滚动窗口（self._ts_window）取值，如果当前窗口内的nan值比例多于75%,此个feature的当前time_id就算作nan；否则进行后面步骤，算具体的值\n",
    "        2. 对每个feature在每个time_id计算MA5（5个time_id的平均，包括当前时间点）和MA15\n",
    "        3. 计算此feature当前time_id内(MA5-MA15)/MA15，算作MA5和MA15的百分比距离\n",
    "        4. 返回data\n",
    "        \"\"\"\n",
    "        data = (\n",
    "            self._data[SpecialCols.id_cols + feats]\n",
    "            if feats is not None\n",
    "            else self._data.copy()\n",
    "        )\n",
    "\n",
    "        # 剔除weight, target,id_col\n",
    "        feat_cols = [\n",
    "            feat\n",
    "            for feat in data.columns\n",
    "            if feat not in SpecialCols.target_cols + SpecialCols.id_cols\n",
    "        ]\n",
    "\n",
    "        # 只选取需要的数据, 因为是日内平均线，所以选取一天即可\n",
    "        data = data.filter((pl.col(SpecialCols.date_id) == self.date_id) & (\n",
    "            pl.col(SpecialCols.time_id) >= self.time_id - 20))\n",
    "\n",
    "        data_ma5 = data.with_columns(\n",
    "            [\n",
    "                pl.col(col)\n",
    "                .rolling_mean(window_size=5)\n",
    "                .over([SpecialCols.symbol_id], order_by=SpecialCols.time_id)\n",
    "                for col in feat_cols\n",
    "            ]\n",
    "        )  # 计算5日移动平均线，包括当前时间点\n",
    "\n",
    "        data_ma15 = data.with_columns(\n",
    "            [\n",
    "                pl.col(col)\n",
    "                .rolling_mean(window_size=15)\n",
    "                .over([SpecialCols.symbol_id], order_by=SpecialCols.time_id)\n",
    "                for col in feat_cols\n",
    "            ]\n",
    "        )  # 计算5日移动平均线，包括当前时间点\n",
    "        # 时序上MA5和MA15的百分比距离；如果没有MA15就是nan\n",
    "        data_macross = data_ma5.with_columns(\n",
    "            [\n",
    "                ((pl.col(col) - data_ma15[col]) / data_ma15[col]).alias(\n",
    "                    col + \"_macross\"\n",
    "                )\n",
    "                for col in feat_cols\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return data_macross.filter((pl.col(SpecialCols.time_id) == self.time_id))\n",
    "\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def ts_percentile(self, feats: List[str] = None) -> pl.DataFrame:\n",
    "        data = (\n",
    "            self._data[SpecialCols.id_cols + feats]\n",
    "            if feats is not None\n",
    "            else self._data.copy()\n",
    "        )\n",
    "        # 剔除weight, target,id_col\n",
    "        feat_cols = [\n",
    "            feat\n",
    "            for feat in data.columns\n",
    "            if feat not in SpecialCols.target_cols + SpecialCols.id_cols\n",
    "        ]\n",
    "\n",
    "        # 只选取需要的数据, 因为是日内进行rank_pct，所以选取一天即可\n",
    "        data = data.filter((pl.col(SpecialCols.date_id) == self.date_id))\n",
    "        # 数据长度不够，就都是Nan\n",
    "        if data[SpecialCols.time_id].n_unique() < self._ts_window:\n",
    "            return data.filter(\n",
    "                pl.col(SpecialCols.time_id) == self.time_id\n",
    "            ).with_columns(\n",
    "                pl.lit(None).cast(pl.Float32).alias(col + \"_percentile\")\n",
    "                for col in feat_cols\n",
    "            )\n",
    "\n",
    "        pct_rank = (\n",
    "            data.with_columns(pl.col(\"time_id\").cast(pl.Int32))\n",
    "            .rolling(\n",
    "                index_column=\"time_id\",\n",
    "                group_by=[SpecialCols.date_id, SpecialCols.symbol_id],\n",
    "                period=f\"{self._ts_window}i\",\n",
    "            )\n",
    "            .agg(\n",
    "                [\n",
    "                    (pl.col(col).rank().last() / self._ts_window)\n",
    "                    .sub(0.5)\n",
    "                    .alias(col + \"_percentile\")\n",
    "                    for col in feat_cols\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        pct_rank = pct_rank.with_columns(pl.col(\"time_id\").cast(pl.Int16))\n",
    "\n",
    "        return pct_rank.filter(pl.col(SpecialCols.time_id) == self.time_id)\n",
    "\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def ts_diff_percentile(self, feats: List[str] = None) -> pl.DataFrame:\n",
    "        data = (\n",
    "            self._data[SpecialCols.id_cols + feats]\n",
    "            if feats is not None\n",
    "            else self._data.copy()\n",
    "        )\n",
    "        # 剔除weight, target,id_col\n",
    "        feat_cols = [\n",
    "            feat\n",
    "            for feat in data.columns\n",
    "            if feat not in SpecialCols.target_cols + SpecialCols.id_cols\n",
    "        ]\n",
    "\n",
    "        # 只选取需要的数据, 因为是日内进行rank_pct，所以选取一天即可\n",
    "        data = data.filter((pl.col(SpecialCols.date_id) == self.date_id))\n",
    "\n",
    "        # 计算diff\n",
    "        data = data.with_columns(\n",
    "            [\n",
    "                pl.col(col)\n",
    "                .diff()\n",
    "                .over([SpecialCols.date_id, SpecialCols.symbol_id], order_by=SpecialCols.time_id)\n",
    "                .alias(col + \"_diff\")\n",
    "                for col in feat_cols\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 数据长度不够，就都是Nan\n",
    "        if data[SpecialCols.time_id].n_unique() < self._ts_window:\n",
    "            return data.filter(\n",
    "                pl.col(SpecialCols.time_id) == self.time_id\n",
    "            ).with_columns(\n",
    "                pl.lit(None).cast(pl.Float32).alias(col + \"_diff_percentile\")\n",
    "                for col in feat_cols\n",
    "            )\n",
    "\n",
    "        # 计算pct_rank\n",
    "        diff_pct_rank = (\n",
    "            data.with_columns(pl.col(\"time_id\").cast(pl.Int32))\n",
    "            .rolling(\n",
    "                index_column=\"time_id\",\n",
    "                group_by=[SpecialCols.date_id, SpecialCols.symbol_id],\n",
    "                period=f\"{self._ts_window}i\",\n",
    "            )\n",
    "            .agg(\n",
    "                [\n",
    "                    (pl.col(col + \"_diff\").rank().last() / self._ts_window)\n",
    "                    .sub(0.5)\n",
    "                    .alias(col + \"_diff\" + \"_percentile\")\n",
    "                    for col in feat_cols\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        diff_pct_rank = diff_pct_rank.with_columns(\n",
    "            pl.col(\"time_id\").cast(pl.Int16))\n",
    "\n",
    "        return diff_pct_rank.filter(pl.col(SpecialCols.time_id) == self.time_id)\n",
    "\n",
    "    # @timing_decorator_with_params(\"hf\")\n",
    "    def ts_zscore(self, feats: List[str] = None) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        目标： 时序上进行zscore，计算均值标准差\n",
    "        1. 对每个特征进行滚动窗口（self._ts_window）取值，如果当前窗口内的nan值多于25%,此个feature的当前time_id就算作nan；否则算具体的值\n",
    "        2. 对每个feature在每个time_id滚动生成mean和std\n",
    "        3. 返回data\n",
    "        \"\"\"\n",
    "        data = (\n",
    "            self._data[SpecialCols.id_cols + feats]\n",
    "            if feats is not None\n",
    "            else self._data.copy()\n",
    "        )\n",
    "        # 剔除weight, target,id_col\n",
    "        feat_cols = [\n",
    "            feat\n",
    "            for feat in data.columns\n",
    "            if feat not in SpecialCols.target_cols + SpecialCols.id_cols\n",
    "        ]\n",
    "\n",
    "        # 只选取需要的数据, 因为是日内进行, 所以选取一天即可\n",
    "        data = data.filter((pl.col(SpecialCols.date_id) == self.date_id) & (\n",
    "            pl.col(SpecialCols.time_id) >= self.time_id - self._ts_window))\n",
    "        max_time_id = data[SpecialCols.time_id].max()\n",
    "\n",
    "        if max_time_id < int(self._ts_window * 0.25) - 1:\n",
    "\n",
    "            return data.filter((pl.col(SpecialCols.date_id) == self.date_id + 1)).with_columns(\n",
    "                [pl.col(col).alias(col + \"_zscore\") for col in feat_cols]\n",
    "            )  # 返回空的\n",
    "        else:\n",
    "            data_mean = data.with_columns(\n",
    "                [\n",
    "                    pl.col(col).mean()\n",
    "                    .over([SpecialCols.symbol_id], order_by=SpecialCols.time_id)\n",
    "                    for col in feat_cols\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            data_std = data.with_columns(\n",
    "                [\n",
    "                    pl.col(col).std()\n",
    "                    .over([SpecialCols.symbol_id], order_by=SpecialCols.time_id)\n",
    "                    for col in feat_cols\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            data_zscore = data.with_columns(\n",
    "                ((pl.col(col) - data_mean[col]) /\n",
    "                 data_std[col]).alias(col + \"_zscore\")\n",
    "                for col in feat_cols\n",
    "            )\n",
    "            return data_zscore.filter((pl.col(SpecialCols.time_id) == self.time_id))\n",
    "\n",
    "            # return data_zscore.filter((pl.col(SpecialCols.time_id) == self.time_id))\n",
    "            # # @timing_decorator_with_params(\"hf\")\n",
    "\n",
    "    def _update_tsmean_std(self, symbols, feats: List[str] = None) -> None:\n",
    "        # 更新tsmean_std对象\n",
    "        for id in symbols:\n",
    "            if f\"{id}_{self.time_id}\" not in self.tsmean_stder:\n",
    "                self.tsmean_stder[f\"{id}_{self.time_id}\"] = TSMEAN_STD(\n",
    "                    instrument_id=id,\n",
    "                    time_id=self.time_id,\n",
    "                    window=30,\n",
    "                    features_n=len(feats),\n",
    "                )\n",
    "        # 取date_id、time_id时刻的数据\n",
    "        data = self._data.filter(\n",
    "            (pl.col(SpecialCols.date_id) == self.date_id)\n",
    "            & (pl.col(SpecialCols.time_id) == self.time_id)\n",
    "        )\n",
    "\n",
    "        # 如果没有数据, 跳过数据更新\n",
    "        if data.shape[0] == 0:\n",
    "            return 0\n",
    "\n",
    "        # 传输数据\n",
    "        for row in data.iter_slices(n_rows=1):\n",
    "            id = row[SpecialCols.symbol_id].item()\n",
    "            # 构建 x 和 y\n",
    "            one_id_data = data.filter(pl.col(SpecialCols.symbol_id) == id)\n",
    "            if one_id_data.shape[0] == 0:\n",
    "                continue\n",
    "            x = one_id_data[feats].to_numpy().reshape((1, -1))\n",
    "            self.tsmean_stder[f\"{id}_{self.time_id}\"].load_new_value(x)\n",
    "\n",
    "    # @ timing_decorator_with_params(\"hf\")\n",
    "    def gen_feats(self):\n",
    "        \"\"\"\n",
    "        return\n",
    "            all_features_data:所用的生成的特征数据\n",
    "            all_selected_feats:使用的生成的特征列表\n",
    "        \"\"\"\n",
    "        # logger.info(\"start computing\")\n",
    "        # 所有选取的特征\n",
    "        all_data = []\n",
    "        flag = False\n",
    "        for func, params in HFDataProcessor.function_to_use.items():\n",
    "            if func in [\"ts_mean_std_same_timeid\", \"ts_regress\"]:\n",
    "                continue\n",
    "            kwargs = {}\n",
    "            kwargs[\"feats\"] = list(set(params[\"func_origin_feats\"]))\n",
    "            if \"_gpby_\" in func:\n",
    "                kwargs[\"groupby_col\"] = func.split(\"_gpby_\")[1]\n",
    "                func = func.split(\"_gpby_\")[0]\n",
    "            data = getattr(self, func)(**kwargs)[\n",
    "                SpecialCols.id_cols + params[\"func_selected_feats\"]\n",
    "            ].sort(SpecialCols.id_cols)\n",
    "            if flag:\n",
    "                all_data.append(data[params[\"func_selected_feats\"]])\n",
    "            else:\n",
    "                all_data.append(data)\n",
    "                flag = True\n",
    "\n",
    "        all_features_data = pl.concat(all_data, how=\"horizontal\")\n",
    "\n",
    "        return all_features_data\n",
    "\n",
    "    # @ timing_decorator_with_params(\"hf\")\n",
    "    def update_original_data(\n",
    "        self, raw_data: pl.DataFrame, lag1d_responder: pl.DataFrame\n",
    "    ):\n",
    "        \"\"\"\n",
    "        data: 原始数据带responder(可以填0)\n",
    "        lag1_responder: column为responder,值滞后一天.\n",
    "        \"\"\"\n",
    "        assert raw_data.shape[1] == 94, \"init_data must be original shape\"\n",
    "        self.cur_processed_raw_data = HFDataProcessor.easy_process_raw_data(\n",
    "            raw_data, lag1d_responder\n",
    "        )\n",
    "\n",
    "        input_len = self.cur_processed_raw_data.shape[0]\n",
    "        # 如果没有数据\n",
    "        if self._data is None:\n",
    "            self._data = self.cur_processed_raw_data[-self._keep_length:]\n",
    "            # self._data_pd = self._trans_raw_data_2pd(\n",
    "            #     self.cur_processed_raw_data[-self._keep_length:]\n",
    "            # )\n",
    "        else:\n",
    "            assert (\n",
    "                self.cur_processed_raw_data.shape[1] == self._data.shape[1]\n",
    "            ), \"must be the same shape\"\n",
    "            if input_len + self._data.shape[0] > self._keep_length:\n",
    "                self._data = pl.concat(\n",
    "                    [\n",
    "                        self._data[-(self._keep_length - input_len):],\n",
    "                        self.cur_processed_raw_data,\n",
    "                    ],\n",
    "                    how=\"vertical\",\n",
    "                )\n",
    "                # self._data_pd = pd.concat(\n",
    "                #     [\n",
    "                #         self._data_pd.iloc[-(self._keep_length - input_len):],\n",
    "                #         self._trans_raw_data_2pd(raw_data),\n",
    "                #     ]\n",
    "                # )\n",
    "            else:\n",
    "                self._data = pl.concat(\n",
    "                    [self._data, self.cur_processed_raw_data], how=\"vertical\"\n",
    "                )\n",
    "                # self._data_pd = pd.concat(\n",
    "                #     [\n",
    "                #         self._data_pd,\n",
    "                #         self._trans_raw_data_2pd(self.cur_processed_raw_data),\n",
    "                #     ]\n",
    "                # )\n",
    "\n",
    "        return\n",
    "\n",
    "    # @ timing_decorator_with_params(\"hf\")\n",
    "    def update_processed_data(self, processed_data: pl.DataFrame):\n",
    "        \"\"\"更新生成的feature\"\"\"\n",
    "        if len(self._cache) >= self._seq_len:\n",
    "            self._cache.pop(0)\n",
    "        # self._cache.append(self.cur_processed_raw_data.join(\n",
    "        #     processed_data, how=\"left\", on=SpecialCols.id_cols))\n",
    "        self._cache.append(\n",
    "            processed_data)\n",
    "        return\n",
    "\n",
    "\n",
    "    def init_data(self, init_data):\n",
    "        # 调整date_id\n",
    "        max_date_id = init_data[SpecialCols.date_id].max()\n",
    "        init_data = init_data.with_columns(\n",
    "            pl.col(SpecialCols.date_id) - max_date_id - 1)\n",
    "\n",
    "        # 构造responder_lag1d 和 row_id 和 is_score\n",
    "        self._data = init_data.with_columns([pl.col(col).shift(1).over([SpecialCols.symbol_id, SpecialCols.time_id], order_by=SpecialCols.date_id).alias(col+\"_lag1d\") for col in SpecialCols.target_cols]).with_columns(\n",
    "            [pl.lit(False).cast(pl.Boolean).alias(\"is_scored\")]\n",
    "        ).with_columns((pl.lit(0)).alias(\"row_id\").cast(pl.UInt32))[self.col_orders]\n",
    "\n",
    "    # @ timing_decorator_with_params(\"hf\")\n",
    "    def update(\n",
    "        self, date_id, time_id, raw_data: pl.DataFrame, lag1d_responder: pl.DataFrame\n",
    "    ):\n",
    "        \"\"\"\n",
    "        update是对外函数，更新最新数据，每一期都调用\n",
    "        \"\"\"\n",
    "        \"\"\"temp_data 只有一个 time_id\"\"\"\n",
    "        self.date_id = date_id\n",
    "        self.time_id = time_id\n",
    "        if time_id == 0:\n",
    "            # lags处理\n",
    "            lag1d_responder.columns = [col.replace('_lag_1', '')\n",
    "                                       for col in lag1d_responder.columns]\n",
    "            self._temp_lag = lag1d_responder\n",
    "\n",
    "        # 日间数据特征计算有关\n",
    "\n",
    "        self.new_symbols = set(raw_data[SpecialCols.symbol_id].unique())\n",
    "        self.update_original_data(raw_data, self._temp_lag.filter(\n",
    "            (pl.col(SpecialCols.time_id) == self.time_id)))\n",
    "        # 计算pca数据\n",
    "        self.pca_data = self.add_pca(self.cur_processed_raw_data.fill_null(0))\n",
    "\n",
    "        processed_data = (\n",
    "            self.gen_feats()\n",
    "        )\n",
    "\n",
    "        # concat pca数据\n",
    "        processed_data = self.pca_data.join(processed_data, on = SpecialCols.id_cols,how='left')\n",
    "\n",
    "        # 保存的是不fillna或者做其他譬如log之类的更改的\n",
    "        self.update_processed_data(processed_data)\n",
    "\n",
    "        # 日间数据特征计算有关\n",
    "        self.symbols = self.new_symbols\n",
    "\n",
    "    # @ timing_decorator_with_params(\"hf\")\n",
    "    def get(self) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        get是对外函数,返回模型训练用的数据,只在特定需要重训练的时候调用\n",
    "        \"\"\"\n",
    "        return_data = pl.concat(self._cache, how='vertical').fill_null(0)\n",
    "        return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0c699c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.684856Z",
     "iopub.status.busy": "2025-01-13T17:40:23.684521Z",
     "iopub.status.idle": "2025-01-13T17:40:23.689565Z",
     "shell.execute_reply": "2025-01-13T17:40:23.688681Z"
    },
    "papermill": {
     "duration": 0.014393,
     "end_time": "2025-01-13T17:40:23.691323",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.676930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init_data = pl.read_parquet(os.path.join(CONFIG.path, \"train.parquet\", \"partition_id=9\", \"part-0.parquet\"))\n",
    "# dp = HFDataProcessor()\n",
    "# dp.init_data(init_data)\n",
    "# del init_data\n",
    "# gc.collect()\n",
    "\n",
    "# last_date, date_cache = None, []\n",
    "# test_path = \"/kaggle/input/aft-js-dataset/synthetic_test.parquet_short\"\n",
    "# lags_path = \"/kaggle/input/aft-js-dataset/synthetic_lag.parquet_short\"\n",
    "# for test_data, _ in tqdm(generate_data_batches(test_path, lags_path)):\n",
    "#     test, lags = test_data\n",
    "#     test = test.with_columns([pl.lit(0).alias(target_col).cast(pl.Float32) for target_col in SpecialCols.target_cols])  # 必须传这个\n",
    "#     date_id, time_id = test['date_id'][0], test['time_id'][0]\n",
    "#     if last_date is None: \n",
    "#         last_date = date_id\n",
    "#     if last_date != date_id and lags is not None:  # 新的一天更新\n",
    "#         _lags = lags.with_columns((pl.col('date_id') - 1).alias('date_id'))\n",
    "#         _lags = _lags.rename({col: col.replace('_lag_1', '') for col in _lags.columns if '_lag_1' in col})\n",
    "#         last_date_all_data = pl.concat(date_cache) # 不含target\n",
    "#         last_date_all_data = last_date_all_data.join(_lags, on=SpecialCols.id_cols, how='left')\n",
    "#         last_date_all_data.write_parquet(f\"/kaggle/working/data_of_{last_date}.parquet\")\n",
    "#         date_cache = []\n",
    "#     dp.update(date_id, time_id, test, lags)\n",
    "#     one_time_data = dp.get()  # get 出来不含target\n",
    "#     # 保存数据\n",
    "#     newly_updated = one_time_data.filter(pl.col(\"time_id\") == time_id).filter(pl.col(\"date_id\") == date_id)  # 不含target\n",
    "#     date_cache.append(newly_updated) # 不含target\n",
    "#     last_date = date_id\n",
    "#     ####\n",
    "#     #### dataloader\n",
    "#     one_time_data = one_time_data.with_columns([pl.lit(0).alias(target_col).cast(pl.Float32) for target_col in SpecialCols.target_cols]).drop(['row_id', 'is_scored'])\n",
    "#     forecast_loader = get_dataloader(one_time_data, step_len=16, get_last_batch=True)\n",
    "#     for combined_batch in forecast_loader:\n",
    "#         combined_batch = combined_batch.float()\n",
    "#         X = combined_batch[:,:,:,:]  # 最后一个Batch的取值\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7479b4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.705255Z",
     "iopub.status.busy": "2025-01-13T17:40:23.704946Z",
     "iopub.status.idle": "2025-01-13T17:40:23.708537Z",
     "shell.execute_reply": "2025-01-13T17:40:23.707821Z"
    },
    "papermill": {
     "duration": 0.012453,
     "end_time": "2025-01-13T17:40:23.710171",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.697718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = dp.get()\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd71988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.724107Z",
     "iopub.status.busy": "2025-01-13T17:40:23.723853Z",
     "iopub.status.idle": "2025-01-13T17:40:23.727231Z",
     "shell.execute_reply": "2025-01-13T17:40:23.726592Z"
    },
    "papermill": {
     "duration": 0.012325,
     "end_time": "2025-01-13T17:40:23.728762",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.716437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = pl.read_parquet(\"/kaggle/input/aft-js-dataset/data_of_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1e337b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.742044Z",
     "iopub.status.busy": "2025-01-13T17:40:23.741800Z",
     "iopub.status.idle": "2025-01-13T17:40:23.745117Z",
     "shell.execute_reply": "2025-01-13T17:40:23.744497Z"
    },
    "papermill": {
     "duration": 0.011662,
     "end_time": "2025-01-13T17:40:23.746622",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.734960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"/kaggle/working/data_columns.pkl\", 'wb') as f:\n",
    "#     pickle.dump(data.columns,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a89a6",
   "metadata": {
    "papermill": {
     "duration": 0.005797,
     "end_time": "2025-01-13T17:40:23.758422",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.752625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**model definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353f90d",
   "metadata": {
    "papermill": {
     "duration": 0.005754,
     "end_time": "2025-01-13T17:40:23.770254",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.764500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2de3e25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.784197Z",
     "iopub.status.busy": "2025-01-13T17:40:23.783935Z",
     "iopub.status.idle": "2025-01-13T17:40:23.801517Z",
     "shell.execute_reply": "2025-01-13T17:40:23.800789Z"
    },
    "papermill": {
     "duration": 0.026798,
     "end_time": "2025-01-13T17:40:23.803028",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.776230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        patience=5,\n",
    "        delta=0,\n",
    "    ):\n",
    "        self.patience = patience\n",
    "        self.best_value = None\n",
    "        self.count = 0\n",
    "        self.best_model = None\n",
    "        self.delta = delta\n",
    "        self.is_earlystop = False\n",
    "\n",
    "    def earlystop(self, value: float, model):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = value\n",
    "            self.best_model = deepcopy(model.state_dict())\n",
    "        elif (self.best_value - value) > self.delta:\n",
    "            self.best_value = value\n",
    "            self.best_model = deepcopy(model.state_dict())\n",
    "            self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "            print(\"EarlyStop Counter: {:02d}\".format(self.count))\n",
    "            if self.count >= self.patience:\n",
    "                self.is_earlystop = True\n",
    "                print(\"# EarlyStop!\")\n",
    "\n",
    "\n",
    "class IncrementalTrainer:\n",
    "\n",
    "    def __init__(self, model, batch_size=242, lr=0.001, patience=5, epoch=100):\n",
    "        self.input_dim = 237\n",
    "        self.patience = patience\n",
    "        self.epoch = epoch\n",
    "        self.model = model\n",
    "        self.early_stop = EarlyStopper(patience=self.patience)\n",
    "        self.device = (\n",
    "            torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.target_pos = -3\n",
    "        self.weight_pos = -10\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = self.loss_fn\n",
    "\n",
    "    def loss_fn(self, pred, label, weight=None):\n",
    "        \"\"\"\n",
    "        pred: [B*N, 1]\n",
    "        label, weight: [B, N]\n",
    "        \"\"\"\n",
    "        # mse\n",
    "        pred = pred.view(-1, 1)\n",
    "        label = label.view(-1, 1)\n",
    "        weight = weight.view(-1, 1)\n",
    "        if weight is not None:\n",
    "            loss = weight * (pred - label) ** 2\n",
    "        else:\n",
    "            loss = (pred - label) ** 2\n",
    "        return torch.mean(loss)\n",
    "\n",
    "    def metric_fn(self, pred, label, weight):\n",
    "        pred = pred.view(-1, 1)\n",
    "        label = label.view(-1, 1)\n",
    "        weight = weight.view(-1, 1)\n",
    "        # weighted mse, weighted r2\n",
    "        # r2 = 1 - sum(w * (y - y_hat) ** 2) / sum(w * y ** 2)\n",
    "        label = torch.clamp(label, -5, 5)\n",
    "        wmse = weight * (pred - label) ** 2\n",
    "        wr2 = 1 - torch.sum(wmse) / torch.sum(weight * label**2)\n",
    "        return torch.mean(wmse), wr2\n",
    "\n",
    "    def _train_epoch(self, data_loader):\n",
    "        self.model.train()\n",
    "        for combined_batch in tqdm(\n",
    "            batch_combine(data_loader, batch_size=self.batch_size, drop_last=True),\n",
    "            desc=\"train epoch\",\n",
    "        ):\n",
    "            combined_batch = combined_batch.float()\n",
    "            X, weight, y = (\n",
    "                combined_batch[:, :, :, : self.input_dim],\n",
    "                combined_batch[:, :, -1, self.weight_pos],\n",
    "                combined_batch[:, :, -1, self.target_pos],\n",
    "            )\n",
    "            label = y.to(self.device)\n",
    "            weight = weight.to(self.device)\n",
    "            preds = self.model.forward(X.to(self.device))\n",
    "            loss = self.criterion(preds, label, weight)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_value_(self.model.parameters(), 3.0)  # 梯度裁剪\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def _test_epoch(self, data_loader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_list = []\n",
    "            valid_loss_list = []\n",
    "            metric_list = []\n",
    "            for combined_batch in tqdm(\n",
    "                batch_combine(data_loader, batch_size=self.batch_size, drop_last=True),\n",
    "                desc=\"test epoch\",\n",
    "            ):\n",
    "                combined_batch = combined_batch.float()\n",
    "                X, weight, y = (\n",
    "                    combined_batch[:, :, :, : self.input_dim],\n",
    "                    combined_batch[:, :, -1, self.weight_pos],\n",
    "                    combined_batch[:, :, -1, self.target_pos],\n",
    "                )\n",
    "                label = y.to(self.device)\n",
    "                weight = weight.to(self.device)\n",
    "                preds = self.model.forward(X.to(self.device))\n",
    "                loss = self.criterion(preds, label, weight)\n",
    "                metric = self.metric_fn(preds, label, weight)\n",
    "                loss_list.append(loss.item())\n",
    "                valid_loss_list.append(metric[0].item())\n",
    "                metric_list.append(metric[1].item())\n",
    "        return np.mean(loss_list), np.mean(valid_loss_list), np.mean(metric_list)\n",
    "\n",
    "    def incremental_update(self, train_loader, valid_loader):\n",
    "        # 这里原本想优先test一次,但是效果不怎么好\n",
    "        for epoch in range(self.epoch):\n",
    "            self._train_epoch(train_loader)\n",
    "            mean_loss = self._test_epoch(valid_loader)\n",
    "            self.early_stop.earlystop(-mean_loss[2], self.model)  # 这里需要max wr2\n",
    "            if self.early_stop.is_earlystop:\n",
    "                break\n",
    "            print(f\"epoch:{epoch}, loss:{mean_loss}\")\n",
    "        self.model.load_state_dict(self.early_stop.best_model)\n",
    "        self.early_stop = EarlyStopper(patience=self.patience)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        print(\"load model at %s\" % path)\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.fitted = True\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c097d7d",
   "metadata": {
    "papermill": {
     "duration": 0.005932,
     "end_time": "2025-01-13T17:40:23.815080",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.809148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**model training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb5022",
   "metadata": {
    "papermill": {
     "duration": 0.005848,
     "end_time": "2025-01-13T17:40:23.826983",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.821135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35237081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:23.840416Z",
     "iopub.status.busy": "2025-01-13T17:40:23.840089Z",
     "iopub.status.idle": "2025-01-13T17:40:25.187040Z",
     "shell.execute_reply": "2025-01-13T17:40:25.186059Z"
    },
    "papermill": {
     "duration": 1.355607,
     "end_time": "2025-01-13T17:40:25.188764",
     "exception": false,
     "start_time": "2025-01-13T17:40:23.833157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at /kaggle/input/aft-js-dataset/gru_model_state.pth\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_feat,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "        layer_type: str,\n",
    "        output_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert layer_type.upper() in [\"RNN\", \"GRU\", \"LSTM\"], (\n",
    "            \"Unexpected layer name: %s\" % layer_type\n",
    "        )\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = getattr(nn, layer_type.upper())(\n",
    "            input_size=d_feat,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, N, S, F = X.shape\n",
    "        X = X.view(-1, S, F)\n",
    "        out, _ = self.rnn(X)\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = RNNModel(237, 128, 2, 0.2, \"gru\", 1)\n",
    "gru_trainer = IncrementalTrainer(model, epoch=1)\n",
    "gru_trainer.load_model(\"/kaggle/input/aft-js-dataset/gru_model_state.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df0edf",
   "metadata": {
    "papermill": {
     "duration": 0.006073,
     "end_time": "2025-01-13T17:40:25.201417",
     "exception": false,
     "start_time": "2025-01-13T17:40:25.195344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91a9744b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:25.215615Z",
     "iopub.status.busy": "2025-01-13T17:40:25.215130Z",
     "iopub.status.idle": "2025-01-13T17:40:25.305702Z",
     "shell.execute_reply": "2025-01-13T17:40:25.304836Z"
    },
    "papermill": {
     "duration": 0.099905,
     "end_time": "2025-01-13T17:40:25.307554",
     "exception": false,
     "start_time": "2025-01-13T17:40:25.207649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model at /kaggle/input/aft-js-dataset/transformer_model_state_3_16_05.pth\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import copy, math\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    Produce N identical layers.\n",
    "    # 实现一个网络的深copy，一个新的对象和原来的对象完全分离，不分享任何存储空间\n",
    "    # 从而保证可训练参数，都有自己的取值，梯度\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # [batch_size, num_head, sequence_len, d_k], batch_size维度是不参与计算的\n",
    "        d_k = query.size(-1)\n",
    "        # KQ点积代表相似度，除以sqrt(d_k)=4防止过大\n",
    "        # [batch_size, num_head, sequence_len, sequence_len]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # 对最后一个维度softmax，作为权重\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        # 加权 [batch_size, num_head, sequence_len, d_k]\n",
    "        output = torch.matmul(p_attn, value)\n",
    "        return output, p_attn  # 返回 p_attn(64,8,30,30)，只是为了可视化多头注意力机制\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_head, d_model):\n",
    "        # num_head 多头数量\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        assert d_model % num_head == 0  # We assume d_v always equals d_k\n",
    "        self.d_k = self.d_model // self.num_head\n",
    "        self.d_v = self.d_model // self.num_head\n",
    "\n",
    "        # 先通过一层linear，然后降维\n",
    "        self.W_Q = nn.Linear(self.d_model, self.d_k * self.num_head, bias=False)\n",
    "        self.W_K = nn.Linear(self.d_model, self.d_k * self.num_head, bias=False)\n",
    "        self.W_V = nn.Linear(self.d_model, self.d_v * self.num_head, bias=False)\n",
    "        self.fc = nn.Linear(self.num_head * self.d_v, self.d_model, bias=False)\n",
    "\n",
    "        self.attention_ = attention()\n",
    "        self.attn = None\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # K Q V = [batch_size, sequence_len, d_model]\n",
    "        nbatches = query.size(0)\n",
    "        # K Q V = [batch_size, n_heads, sequence_len, d_k]\n",
    "        query = (\n",
    "            self.W_Q(query).view(nbatches, -1, self.num_head, self.d_k).transpose(1, 2)\n",
    "        )\n",
    "        key = self.W_K(key).view(nbatches, -1, self.num_head, self.d_k).transpose(1, 2)\n",
    "        value = (\n",
    "            self.W_V(value).view(nbatches, -1, self.num_head, self.d_v).transpose(1, 2)\n",
    "        )\n",
    "        # x = [batch_size, n_heads, sequence_len, d_k]\n",
    "        x, self.attn = self.attention_(query, key, value)\n",
    "        # 维度转换 x = [batch_size, sequence_len, d_k*n_heads]\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.num_head * self.d_k)\n",
    "        # 线性层变回 [batch_size, sequence_len, d_model]\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    # 全连接网络，包含两个线性变换(注意带有常数偏离项)和一个非线性函数(如ReLU)\n",
    "    # linear -> relu -> linear\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        # [batch_size, sequence_len, d_model]\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        # [batch_size, sequence_len, d_ff]\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        # [batch_size, sequence_len, d_model]\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.w_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)  # size=d_model=512\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        # x [batch_size, sequence_len, d_model]\n",
    "        # sublayer是一个具体的MultiHeadAttention 或 PositionwiseFeedForward对象\n",
    "        x = self.norm(inputs)\n",
    "        x = sublayer(x)\n",
    "        x = inputs + self.dropout(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)  # 深度克隆两个\n",
    "        self.size = size  # 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers):\n",
    "        \"\"\"\n",
    "        Encoder is a stack of N layers\n",
    "        layer: instance of EncoderLayer\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [batch_size, sequence_len, d_model]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        # [batch_size, sequence_len, d_model]\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer's Encoder part. Base for this and many other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_feat,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        num_heads,\n",
    "        output_size,\n",
    "        dropout,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        task_type: \"binary\", \"multiclass\", \"regression\"\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_feat = d_feat\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_heads = num_heads\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        self.embeding = nn.Linear(self.d_feat, self.d_model)\n",
    "        self.attn = MultiHeadedAttention(self.num_heads, self.d_model)\n",
    "        self.ff = PositionwiseFeedForward(self.d_model, self.d_ff)\n",
    "        self.encoderlayer = EncoderLayer(self.d_model, self.attn, self.ff, self.dropout)\n",
    "        self.encoder = Encoder(self.encoderlayer, self.num_layers)\n",
    "        self.init_weights()  # normal initialization\n",
    "        self.outputs = nn.Linear(self.d_model, self.output_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.trunc_normal_(p, mean=0, std=0.01, a=-0.1, b=0.1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        B, N = src.shape[0], src.shape[1]\n",
    "        src = src[:, :, -1, :]  # 截面\n",
    "        src = self.embeding(src)\n",
    "        src = self.encoder(src)\n",
    "        x_hidden = src.reshape(B, N, -1)\n",
    "        res = self.outputs(x_hidden)\n",
    "        return res\n",
    "\n",
    "model = Transformer(3, 237, 128, 128, 16, 1, 0.5)\n",
    "transformer_trainer = IncrementalTrainer(model, epoch=1)\n",
    "transformer_trainer.load_model(\"/kaggle/input/aft-js-dataset/transformer_model_state_3_16_05.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d608b",
   "metadata": {
    "papermill": {
     "duration": 0.006027,
     "end_time": "2025-01-13T17:40:25.320013",
     "exception": false,
     "start_time": "2025-01-13T17:40:25.313986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef02d2c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:25.334051Z",
     "iopub.status.busy": "2025-01-13T17:40:25.333701Z",
     "iopub.status.idle": "2025-01-13T17:40:25.339788Z",
     "shell.execute_reply": "2025-01-13T17:40:25.339034Z"
    },
    "papermill": {
     "duration": 0.015045,
     "end_time": "2025-01-13T17:40:25.341273",
     "exception": false,
     "start_time": "2025-01-13T17:40:25.326228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class MYMLP(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         # Time-series MLP\n",
    "#         self.time_series_mlp = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Cross-sectional MLP\n",
    "#         self.cross_sectional_mlp = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         \"\"\"\n",
    "#         [batch_size, cross_number, time_steps, feature_size] -> (batch_size x cross_number, output_dim)\n",
    "#         \"\"\"\n",
    "#         batch_size, cross_number, time_steps, feature_size = data.shape\n",
    "\n",
    "#         # Process time-series information\n",
    "#         data = data.view(batch_size * cross_number, time_steps, feature_size)\n",
    "#         time_features = self.time_series_mlp(data.view(-1, feature_size))  # [(batch_size * cross_number * time_steps) x hidden_dim]\n",
    "#         time_features = time_features.view(batch_size * cross_number, time_steps, -1).mean(dim=1)  # Aggregate across time steps\n",
    "\n",
    "#         # Reshape back to cross-sectional structure\n",
    "#         time_features = time_features.view(batch_size, cross_number, -1)  # Shape: [batch_size, cross_number, hidden_dim]\n",
    "\n",
    "#         # Process cross-sectional features\n",
    "#         cross_features = self.cross_sectional_mlp(time_features)  # Shape: [batch_size, cross_number, output_dim]\n",
    "#         return cross_features\n",
    "# # Model configuration\n",
    "# input_dim = input_dim\n",
    "# hidden_dim = 32\n",
    "# output_dim = 1\n",
    "# model = MYMLP(input_dim, hidden_dim, output_dim)\n",
    "# # trainer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# trainer = IncrementalTrainer(model, optimizer, criterion)\n",
    "# # training\n",
    "# step_len = 16\n",
    "# batch_size = 242\n",
    "# start = time.time()\n",
    "# exist_data = pl.read_parquet(\"/kaggle/input/aft-js-dataset/data_of_0.parquet\").drop(['row_id', 'is_scored'])\n",
    "# # # 计算每列的均值和标准差\n",
    "# # all_feature = list(set(exist_data.columns)-set([SpecialCols.weight_col]+SpecialCols.target_cols+SpecialCols.id_cols))\n",
    "# # features = exist_data[all_feature].to_numpy()\n",
    "# # means, stds = np.mean(features, axis=0), np.std(features, axis=0)\n",
    "# # stds = np.where(stds < 1e-6, 1e-6, stds)\n",
    "# # exist_data = standardize_polars(exist_data, means, stds)\n",
    "# ###\n",
    "# train_loader = get_dataloader(exist_data, step_len)\n",
    "# test_loader = get_dataloader(exist_data, step_len)\n",
    "# trainer.incremental_update(train_loader, test_loader)\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e747005",
   "metadata": {},
   "source": [
    "MASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39662ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.shape[2], :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "class SAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.temperature = math.sqrt(self.d_model/nhead)\n",
    "\n",
    "        self.qtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.ktrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.vtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        attn_dropout_layer = []\n",
    "        for i in range(nhead):\n",
    "            attn_dropout_layer.append(Dropout(p=dropout))\n",
    "        self.attn_dropout = nn.ModuleList(attn_dropout_layer)\n",
    "\n",
    "        # input LayerNorm\n",
    "        self.norm1 = LayerNorm(d_model, eps=1e-5)\n",
    "        # self.norm1 = BatchNorm1d(d_model, eps=1e-5)\n",
    "\n",
    "\n",
    "        # FFN layerNorm\n",
    "        self.norm2 = LayerNorm(d_model, eps=1e-5)\n",
    "        # self.norm2 = BatchNorm1d(d_model, eps=1e-5)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            Dropout(p=dropout),\n",
    "            Linear(d_model, d_model),\n",
    "            Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, N, T, D = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(-1, N, D) # [batch, N, T, D] --> [batch, T, N, D]\n",
    "        #x = x.permute(0, 2, 3, 1).contiguous().view(-1, D, N) # [batch, N, T, D] --> [batch, T, D, N] --> [batch * T, D, N]\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        q = self.qtrans(x)\n",
    "        k = self.ktrans(x)\n",
    "        v = self.vtrans(x)\n",
    "\n",
    "        dim = int(self.d_model/self.nhead)\n",
    "        att_output = []\n",
    "        for i in range(self.nhead):\n",
    "            if i==self.nhead-1:\n",
    "                qh = q[:, :, i * dim:]\n",
    "                kh = k[:, :, i * dim:]\n",
    "                vh = v[:, :, i * dim:]\n",
    "            else:\n",
    "                qh = q[:, :, i * dim:(i + 1) * dim]\n",
    "                kh = k[:, :, i * dim:(i + 1) * dim]\n",
    "                vh = v[:, :, i * dim:(i + 1) * dim]\n",
    "\n",
    "            atten_ave_matrixh = torch.softmax(torch.matmul(qh, kh.transpose(1, 2)) / self.temperature, dim=-1) # [batch * T, N, D] * [batch * T, D, N] --> [batch * T, N, N]\n",
    "            if self.attn_dropout:\n",
    "                atten_ave_matrixh = self.attn_dropout[i](atten_ave_matrixh) \n",
    "            att_output.append(torch.matmul(atten_ave_matrixh, vh)) # [batch *T, N, N] * [batch *T, N, D] --> [batch *T, N, D]\n",
    "        att_output = torch.concat(att_output, dim=-1)\n",
    "\n",
    "        # FFN\n",
    "        xt = x + att_output\n",
    "        xt = self.norm2(xt)\n",
    "        att_output = xt + self.ffn(xt)\n",
    "\n",
    "        return att_output.view(batch, T, N, D).permute(0, 2, 1, 3).contiguous() # [batch * T, N, D] --> [batch, T, N, D] --> [batch, N, T, D]\n",
    "\n",
    "\n",
    "class TAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.qtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.ktrans = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.vtrans = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.attn_dropout = []\n",
    "        if dropout > 0:\n",
    "            for i in range(nhead):\n",
    "                self.attn_dropout.append(Dropout(p=dropout))\n",
    "            self.attn_dropout = nn.ModuleList(self.attn_dropout)\n",
    "\n",
    "        # input LayerNorm\n",
    "        self.norm1 = LayerNorm(d_model, eps=1e-5)\n",
    "        # self.norm1 = BatchNorm1d(d_model, eps=1e-5)\n",
    "\n",
    "        # FFN layerNorm\n",
    "        self.norm2 = LayerNorm(d_model, eps=1e-5)\n",
    "        # self.norm2 = BatchNorm1d(d_model, eps=1e-5)\n",
    "\n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            Dropout(p=dropout),\n",
    "            Linear(d_model, d_model),\n",
    "            Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, N, T, D = x.shape\n",
    "        x = x.view(-1, T, D)\n",
    "        x = self.norm1(x)\n",
    "        q = self.qtrans(x)\n",
    "        k = self.ktrans(x)\n",
    "        v = self.vtrans(x)\n",
    "\n",
    "        dim = int(self.d_model / self.nhead)\n",
    "        att_output = []\n",
    "        for i in range(self.nhead):\n",
    "            if i==self.nhead-1:\n",
    "                qh = q[:, :, i * dim:]\n",
    "                kh = k[:, :, i * dim:]\n",
    "                vh = v[:, :, i * dim:]\n",
    "            else:\n",
    "                qh = q[:, :, i * dim:(i + 1) * dim]\n",
    "                kh = k[:, :, i * dim:(i + 1) * dim]\n",
    "                vh = v[:, :, i * dim:(i + 1) * dim]\n",
    "            atten_ave_matrixh = torch.softmax(torch.matmul(qh, kh.transpose(1, 2)), dim=-1) # [N, T, dim] * [N, dim, T] --> [N, T, T]\n",
    "            if self.attn_dropout:\n",
    "                atten_ave_matrixh = self.attn_dropout[i](atten_ave_matrixh)\n",
    "            att_output.append(torch.matmul(atten_ave_matrixh, vh)) # [N, T, T] * [N, T, dim] --> [N, T, dim]\n",
    "        att_output = torch.concat(att_output, dim=-1) # [N, T, D]\n",
    "\n",
    "        # FFN\n",
    "        xt = x + att_output\n",
    "        xt = self.norm2(xt)\n",
    "        att_output = xt + self.ffn(xt)\n",
    "\n",
    "        return att_output.view(batch, N, T, D)\n",
    "\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.trans = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch, N, T, D = z.shape\n",
    "        z = z.view(-1, T, D)\n",
    "        h = self.trans(z) # [batch * N, T, D]\n",
    "        query = h[:, -1, :].unsqueeze(-1)\n",
    "        lam = torch.matmul(h, query).squeeze(-1)  # [batch * N, T, D] --> [batch * N, T]\n",
    "        lam = torch.softmax(lam, dim=1).unsqueeze(1) # [batch * N, T] --> [batch * N, 1, T]\n",
    "        output = torch.matmul(lam, z).squeeze(1)  # [batch * N, 1, T], [batch * N, T, D] --> [batch * N, 1, D] --> [batch * N, D]\n",
    "        return output.view(batch, N, D)\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, d_input, d_output, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.trans = nn.Linear(d_input, d_output)\n",
    "        self.d_output =d_output\n",
    "        self.t = beta\n",
    "\n",
    "    def forward(self, gate_input):\n",
    "        output = self.trans(gate_input)\n",
    "        output = torch.softmax(output/self.t, dim=-1)\n",
    "        return self.d_output*output\n",
    "\n",
    "class MASTER(nn.Module):\n",
    "    def __init__(self, d_feat=76, d_model=256, t_nhead=2, s_nhead=2, T_dropout_rate=0.5, S_dropout_rate=0.5,\n",
    "                 gate_input_start_index=None, gate_input_end_index=None, beta=None):\n",
    "        super(MASTER, self).__init__()\n",
    "        #self.gate_input_start_index = gate_input_start_index\n",
    "        #self.gate_input_end_index = gate_input_end_index\n",
    "        #self.d_gate_input = gate_input_end_index - gate_input_start_index\n",
    "        #self.feature_gate = Gate(self.d_gate_input, d_feat, beta=beta)\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            # feature layer\n",
    "            nn.Linear(d_feat, d_model),\n",
    "            PositionalEncoding(d_model),\n",
    "            # intra-stock aggregation\n",
    "            TAttention(d_model=d_model, nhead=t_nhead, dropout=T_dropout_rate),\n",
    "            # inter-stock aggregation\n",
    "            SAttention(d_model=d_model, nhead=s_nhead, dropout=S_dropout_rate),\n",
    "            TemporalAttention(d_model=d_model),\n",
    "            # decoder\n",
    "            nn.Linear(d_model, 1),\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        src = x\n",
    "        #src = x[:, :, :, :self.gate_input_start_index]\n",
    "        #gate_input = x[:, :, -1, self.gate_input_start_index:]\n",
    "        #src = src * torch.unsqueeze(self.feature_gate(gate_input), dim=2)\n",
    "        output = self.layers(src).squeeze(-1)\n",
    "        # clip to [-5, 5]\n",
    "        # output = output - torch.mean(output, dim=1, keepdim=True)\n",
    "        output = torch.clamp(output, -5.0, 5.0)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f99cd3a",
   "metadata": {
    "papermill": {
     "duration": 0.006038,
     "end_time": "2025-01-13T17:40:25.353446",
     "exception": false,
     "start_time": "2025-01-13T17:40:25.347408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd18e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:25.367473Z",
     "iopub.status.busy": "2025-01-13T17:40:25.367111Z",
     "iopub.status.idle": "2025-01-13T17:40:30.739200Z",
     "shell.execute_reply": "2025-01-13T17:40:30.737992Z"
    },
    "papermill": {
     "duration": 5.380981,
     "end_time": "2025-01-13T17:40:30.741023",
     "exception": false,
     "start_time": "2025-01-13T17:40:25.360042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Ignoring unrecognized parameter 'early_stopping_min_delta' found in model string.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model =lgb.Booster(model_file=\"/kaggle/input/aft-js-dataset/lgb_1_14.lgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5336f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:30.755659Z",
     "iopub.status.busy": "2025-01-13T17:40:30.754660Z",
     "iopub.status.idle": "2025-01-13T17:40:30.767716Z",
     "shell.execute_reply": "2025-01-13T17:40:30.767040Z"
    },
    "papermill": {
     "duration": 0.021759,
     "end_time": "2025-01-13T17:40:30.769245",
     "exception": false,
     "start_time": "2025-01-13T17:40:30.747486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_feature = pd.read_pickle(\"/kaggle/input/aft-js-dataset/data_columns.pkl\")[6:-9]\n",
    "means, stds = pd.read_pickle(\"/kaggle/input/aft-js-dataset/mean_std.pkl\")\n",
    "def standardize_polars(data: pl.DataFrame, means=means, stds=stds):\n",
    "    features = data[all_feature].to_numpy()\n",
    "    data_standardized = np.clip((features - means) / stds, -5, 5)\n",
    "    data = data.with_columns(pl.Series(data_standardized[:, i]).alias(f) for i, f in enumerate(all_feature))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0adff",
   "metadata": {
    "papermill": {
     "duration": 0.006083,
     "end_time": "2025-01-13T17:40:30.781794",
     "exception": false,
     "start_time": "2025-01-13T17:40:30.775711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**初始模型和dp都定义好后，开始online training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a759fc46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:30.795649Z",
     "iopub.status.busy": "2025-01-13T17:40:30.795303Z",
     "iopub.status.idle": "2025-01-13T17:40:30.799665Z",
     "shell.execute_reply": "2025-01-13T17:40:30.798753Z"
    },
    "papermill": {
     "duration": 0.01348,
     "end_time": "2025-01-13T17:40:30.801456",
     "exception": false,
     "start_time": "2025-01-13T17:40:30.787976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_one_batch(data, sequence_len, feature_dim):\n",
    "#     symbols = data.filter(pl.col(\"time_id\") == data['time_id'].max())['symbol_id'].to_list()\n",
    "#     output_array = np.zeros((1, len(symbols), sequence_len, feature_dim))\n",
    "#     for i, sym in enumerate(symbols):\n",
    "#         symbol_data = data.filter(pl.col(\"symbol_id\") == sym).tail(sequence_len).drop(SpecialCols.id_cols+['weight'])\n",
    "#         output_array[0, i, -len(symbol_data):, :] = symbol_data.to_numpy()[:, :]\n",
    "#     return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6e1275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:30.818985Z",
     "iopub.status.busy": "2025-01-13T17:40:30.818721Z",
     "iopub.status.idle": "2025-01-13T17:40:30.837165Z",
     "shell.execute_reply": "2025-01-13T17:40:30.836498Z"
    },
    "papermill": {
     "duration": 0.028419,
     "end_time": "2025-01-13T17:40:30.838831",
     "exception": false,
     "start_time": "2025-01-13T17:40:30.810412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OnlineTraining:\n",
    "    def __init__ (self, data_processor: HFDataProcessor, models: List):\n",
    "        \"\"\"\n",
    "        models: 模型列表 也可以是树模型\n",
    "        \"\"\"\n",
    "        self.dp = data_processor\n",
    "        self.lags_ = None  # 滞后responder\n",
    "        \n",
    "        self.models = models\n",
    "        self.retrain_days = 7 # 累计多少天数据更新一次 至少两天\n",
    "        self.days_passed = 0\n",
    "        self.last_date = None\n",
    "        self.date_cache = []\n",
    "\n",
    "        self.step_len = 16  # 固定的\n",
    "        self.input_dim = 237  # 特征维度\n",
    "        self.responder6_pos = -3  # responder6的位置\n",
    "\n",
    "        self.pbar = tqdm(desc=\"test prediction\")\n",
    "\n",
    "        self.test_parquet = '/kaggle/input/aft-js-dataset/synthetic_test.parquet_short'\n",
    "        self.lag_parquet = '/kaggle/input/aft-js-dataset/synthetic_lag.parquet_short'\n",
    "        \n",
    "\n",
    "    def run_inference_server(self):\n",
    "        inference_server = js_server.JSInferenceServer(self.predict)\n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            inference_server.serve()\n",
    "        else:\n",
    "            inference_server.run_local_gateway((self.test_parquet, self.lag_parquet))\n",
    "\n",
    "    \n",
    "    def predict(self, test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "        predictions = test.select(['row_id'])\n",
    "        is_score = test['is_scored'].any()\n",
    "        if not is_score:  # 全都不需要预测\n",
    "            predictions = predictions.with_columns(pl.lit(0).alias('responder_6'))\n",
    "            return predictions\n",
    "\n",
    "        date_id, time_id = test['date_id'][0], test['time_id'][0]\n",
    "        \n",
    "        if self.last_date is None: \n",
    "            self.last_date = date_id\n",
    "        \n",
    "        # 增量训练\n",
    "        if self.last_date != date_id and lags is not None:  # 新的一天更新\n",
    "            start = time.time()\n",
    "            _lags = lags.with_columns(pl.col('date_id') - 1)\n",
    "            _lags = _lags.rename({col: col.replace('_lag_1', '') for col in _lags.columns if '_lag_1' in col})\n",
    "            last_date_all_data = pl.concat(self.date_cache)  # .with_columns([pl.col(f).cast(_lags[f].dtype) for f in SpecialCols.id_cols])\n",
    "            last_date_all_data = last_date_all_data.join(_lags, on=SpecialCols.id_cols, how='left')\n",
    "            last_date_all_data.write_parquet(f\"/kaggle/working/data_of_{self.last_date}.parquet\")\n",
    "            self.date_cache = []\n",
    "            self.days_passed += 1\n",
    "            if self.days_passed >= self.retrain_days:  # 达到重训练天数\n",
    "                print(\"start retraining\")\n",
    "                self.days_passed = 0\n",
    "                # 增量训练\n",
    "                too_old_file = f\"/kaggle/working/data_of_{date_id-self.retrain_days-1}.parquet\"\n",
    "                if os.path.exists(too_old_file): # 移除早期文件, 避免/kaggle/working/文件夹爆了\n",
    "                    os.remove(too_old_file)\n",
    "                retrain_data_train = pl.concat([pl.read_parquet(f\"/kaggle/working/data_of_{date_id-d}.parquet\") for d in range(2, 1+self.retrain_days)], how='vertical').drop(['row_id', 'is_scored'])\n",
    "                retrain_data_valid = pl.read_parquet(f\"/kaggle/working/data_of_{date_id-1}.parquet\").drop(['row_id', 'is_scored'])\n",
    "                ### standardized\n",
    "                retrain_data_train = standardize_polars(retrain_data_train)\n",
    "                retrain_data_valid = standardize_polars(retrain_data_valid)\n",
    "                ################\n",
    "                train_loader = get_dataloader(retrain_data_train, self.step_len)\n",
    "                test_loader = get_dataloader(retrain_data_valid, self.step_len)\n",
    "                for trainer in self.models:\n",
    "                    if isinstance(trainer, IncrementalTrainer):\n",
    "                        trainer.incremental_update(train_loader, test_loader)\n",
    "                gc.collect()\n",
    "            end = time.time()\n",
    "            print(end-start)\n",
    "\n",
    "        # 数据更新\n",
    "        test = test.with_columns([pl.lit(0).alias(f) for f in SpecialCols.target_cols])\n",
    "        self.dp.update(date_id, time_id, test, lags)  # 更新最新数据\n",
    "        one_time_data = self.dp.get()  # 不含target\n",
    "        newly_updated = one_time_data.filter(pl.col(\"time_id\") == time_id).filter(pl.col(\"date_id\") == date_id)\n",
    "        self.date_cache.append(newly_updated)\n",
    "        self.last_date = date_id\n",
    "\n",
    "        # 数据预测\n",
    "        one_time_data = one_time_data.with_columns([pl.lit(0).alias(t) for t in SpecialCols.target_cols]).drop(['is_scored', 'row_id'])\n",
    "        one_time_data = standardize_polars(one_time_data)\n",
    "        forecast_loader = get_dataloader(one_time_data, step_len=self.step_len, get_last_batch=True)\n",
    "        for combined_batch in forecast_loader:\n",
    "            combined_batch = combined_batch.float()\n",
    "            X = combined_batch[:,:,:,:self.input_dim]  # 最后一个Batch的取值\n",
    "            break\n",
    "            \n",
    "        # # get one batch\n",
    "        # symbols = newly_updated['symbol_id'].to_list()\n",
    "        # X = np.zeros((1, len(symbols), self.step_len, self.input_dim))\n",
    "        # for i, sym in enumerate(symbols):\n",
    "        #     symbol_data = one_time_data.filter(pl.col(\"symbol_id\") == sym).tail(self.step_len).drop(SpecialCols.id_cols).to_numpy()\n",
    "        #     X[0, i, -len(symbol_data):, :] = symbol_data\n",
    "        # X = torch.tensor(X).float()\n",
    "            \n",
    "        cnt, res = 0, 0\n",
    "        for trainer in self.models:  # # ensemble\n",
    "            if isinstance(trainer, IncrementalTrainer):\n",
    "                preds = trainer.model(X.to(trainer.device)) # [1, cross_number, output_dim]\n",
    "            else:\n",
    "                # 树模型\n",
    "                preds = trainer.predict(X[0,:,-1,:self.input_dim].detach().numpy())\n",
    "                \n",
    "            if isinstance(preds, torch.Tensor):\n",
    "                if 'cpu' not in str(trainer.device):\n",
    "                    preds = preds.cpu()\n",
    "                preds = preds.detach().numpy().reshape(-1)\n",
    "            elif isinstance(preds, (pd.DataFrame, pd.Series)):\n",
    "                preds = preds.values.reshape(-1)\n",
    "            else:\n",
    "                preds = preds.reshape(-1)\n",
    "                \n",
    "            res += preds\n",
    "            cnt += 1\n",
    "        preds = res / cnt\n",
    "\n",
    "        # 结果返回\n",
    "        try:\n",
    "            predictions = predictions.with_columns(pl.Series(np.clip(preds, a_min = -5, a_max = 5)).alias('responder_6'))\n",
    "        except:\n",
    "            print(\"shape cannot match.\")\n",
    "            predictions = predictions.with_columns(pl.lit(0).alias('responder_6'))\n",
    "        self.pbar.update(1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7b32001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T17:40:30.852826Z",
     "iopub.status.busy": "2025-01-13T17:40:30.852531Z",
     "iopub.status.idle": "2025-01-13T17:51:19.842477Z",
     "shell.execute_reply": "2025-01-13T17:51:19.841548Z"
    },
    "papermill": {
     "duration": 648.999338,
     "end_time": "2025-01-13T17:51:19.844488",
     "exception": false,
     "start_time": "2025-01-13T17:40:30.845150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test prediction: 969it [02:38,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0060441493988037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test prediction: 1937it [05:18,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9282479286193848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test prediction: 2904it [08:00,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9317927360534668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test prediction: 3872it [10:48,  5.74it/s]"
     ]
    }
   ],
   "source": [
    "# init_data = pl.read_parquet(os.path.join(CONFIG.path, \"train.parquet\", \"partition_id=9\", \"part-0.parquet\"))\n",
    "dp = HFDataProcessor()\n",
    "# dp.init_data(init_data)\n",
    "# del init_data\n",
    "# gc.collect()\n",
    "js_predictor = OnlineTraining(dp, [gru_trainer, transformer_trainer, lgb_model])\n",
    "js_predictor.run_inference_server()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "datasetId": 6323508,
     "sourceId": 10461913,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 666.781491,
   "end_time": "2025-01-13T17:51:22.139958",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-13T17:40:15.358467",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
